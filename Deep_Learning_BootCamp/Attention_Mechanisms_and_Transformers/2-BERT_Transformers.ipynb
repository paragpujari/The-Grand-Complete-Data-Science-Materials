{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8f8987a",
      "metadata": {
        "id": "a8f8987a"
      },
      "source": [
        "\n",
        "# BERT Transformers\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· What is Transformer?\n",
        "\n",
        "Transformer is a deep learning architecture introduced in 2017 in the paper:\n",
        "\"Attention Is All You Need\".\n",
        "\n",
        "### Key Features:\n",
        "- Uses Self-Attention mechanism\n",
        "- Processes entire sequence in parallel\n",
        "- Handles long-range dependencies efficiently\n",
        "- Replaced RNNs & LSTMs in NLP tasks\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· What is BERT?\n",
        "\n",
        "BERT = Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "It is a pretrained Transformer Encoder model developed by Google.\n",
        "\n",
        "### Key Characteristics:\n",
        "- Bidirectional (reads text left & right simultaneously)\n",
        "- Pretrained on massive text corpus\n",
        "- Fine-tuned for downstream NLP tasks\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· Main Functions of BERT\n",
        "\n",
        "1. Text Classification\n",
        "2. Sentiment Analysis\n",
        "3. Question Answering\n",
        "4. Named Entity Recognition\n",
        "5. Text Similarity\n",
        "6. Language Understanding Tasks\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”· Steps used in this Algorithm:-\n",
        "\n",
        "1.  Import all the necessary libraries\n",
        "\n",
        "2.  Load the imdb Dataset\n",
        "\n",
        "3.  Load the BERT Tokenizer\n",
        "\n",
        "4.  Perform the Tokenization\n",
        "\n",
        "5.  Convert to TensorFlow Dataset\n",
        "\n",
        "6.  Load the  Pretrained BERT Model\n",
        "\n",
        "7.  Compile the BERT Model\n",
        "\n",
        "8.  Train the BERT Model\n",
        "\n",
        "9.  Evaluate the BERT Model\n",
        "\n",
        "10. Plot Accuracy vs Loss\n",
        "\n",
        "11. Perform Predictions on the sample text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import all the necessary libraries"
      ],
      "metadata": {
        "id": "tay0GtJ941or"
      },
      "id": "tay0GtJ941or"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8fXhdUUM5jAU"
      },
      "id": "8fXhdUUM5jAU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **OUTPUT:**\n",
        "\n",
        "| Library                         | Purpose                     |\n",
        "| ------------------------------- | --------------------------- |\n",
        "| BertTokenizer                   | Converts text â†’ BERT format |\n",
        "| TFBertForSequenceClassification | Pretrained BERT model       |\n",
        "| load_dataset                    | Loads IMDB                  |\n",
        "| tensorflow                      | Training                    |\n"
      ],
      "metadata": {
        "id": "tdKsksI546rp"
      },
      "id": "tdKsksI546rp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load the imdb Dataset"
      ],
      "metadata": {
        "id": "A_5oD8La498J"
      },
      "id": "A_5oD8La498J"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_texts = dataset['train']['text'][:2000]\n",
        "train_labels = dataset['train']['label'][:2000]\n",
        "\n",
        "test_texts = dataset['test']['text'][:500]\n",
        "test_labels = dataset['test']['label'][:500]\n"
      ],
      "metadata": {
        "id": "xeY0yc7n4_6W"
      },
      "id": "xeY0yc7n4_6W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.  We have an IMDB Dataset that has 50,000 movie reviews and is used for Binary classification.\n",
        "\n",
        "### **OUTPUT:**\n",
        "\n",
        "| Label | Meaning  |\n",
        "| ----- | -------- |\n",
        "| 0     | Negative |\n",
        "| 1     | Positive |\n",
        "\n",
        "2.  We take subset for faster training"
      ],
      "metadata": {
        "id": "1f-vfymo5Fsl"
      },
      "id": "1f-vfymo5Fsl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Load the BERT Tokenizer"
      ],
      "metadata": {
        "id": "wxp3EiV35ouy"
      },
      "id": "wxp3EiV35ouy"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "KaQ-1xgz5qtE"
      },
      "id": "KaQ-1xgz5qtE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.  What happens internally?\n",
        "\n",
        "It Loads:\n",
        "\n",
        "(a.)   Vocabulary (30,522 tokens)\n",
        "\n",
        "(b.)   WordPiece tokenizer\n",
        "\n",
        "(c.)   Special tokens:\n",
        "\n",
        "        (i.)         [CLS]\n",
        "\n",
        "        (ii.)        [SEP]\n",
        "\n",
        "        (iii.)       [PAD]"
      ],
      "metadata": {
        "id": "TeITsaVV5vwc"
      },
      "id": "TeITsaVV5vwc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Perform the Tokenization"
      ],
      "metadata": {
        "id": "SNVIyDsS6GK9"
      },
      "id": "SNVIyDsS6GK9"
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    test_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n"
      ],
      "metadata": {
        "id": "K_5z-Nl76JBt"
      },
      "id": "K_5z-Nl76JBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.     It   Cuts long reviews beyond 128 tokens.\n",
        "\n",
        "2.    It performs the Padding in  shorter sentences to 128.\n",
        "\n",
        "3.   All inputs become shape: (batch_size, 128)\n",
        "\n",
        "4.   Every sentence becomes:  [CLS] I love this movie [SEP]\n",
        "\n",
        "And converted into:\n",
        "\n",
        "    (a.)   input_ids\n",
        "\n",
        "    (b.)   attention_mask\n",
        "\n",
        "    (c.)   token_type_ids"
      ],
      "metadata": {
        "id": "WPzqcR6Y6PxY"
      },
      "id": "WPzqcR6Y6PxY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Convert to TensorFlow Dataset"
      ],
      "metadata": {
        "id": "rbYiEko56-d-"
      },
      "id": "rbYiEko56-d-"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).shuffle(1000).batch(16)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(16)\n"
      ],
      "metadata": {
        "id": "HXiqQb4A7ANJ"
      },
      "id": "HXiqQb4A7ANJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.  It  Converts dictionary into tf format\n",
        "\n",
        "        (a.)      shuffle() â†’ Avoids bias\n",
        "\n",
        "        (b.)      batch(16) â†’ Memory efficient"
      ],
      "metadata": {
        "id": "kf8b-Kkd7EHm"
      },
      "id": "kf8b-Kkd7EHm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Load the  Pretrained BERT Model"
      ],
      "metadata": {
        "id": "wTYZrTQM7QlE"
      },
      "id": "wTYZrTQM7QlE"
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "Abo73FPS7SqN"
      },
      "id": "Abo73FPS7SqN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "\n",
        "Text\n",
        " â†“\n",
        "Embedding Layer\n",
        " â†“\n",
        "12 Transformer Encoder Blocks\n",
        " â†“\n",
        "[CLS] token representation\n",
        " â†“\n",
        "Dropout\n",
        " â†“\n",
        "Dense Layer (2 neurons)\n",
        " â†“\n",
        "Logits\n",
        "\n",
        "\n",
        "The model is already pretrained on:\n",
        "\n",
        "    (a.)    Wikipedia\n",
        "    \n",
        "    (b.)    BooksCorpus\n",
        "\n",
        "Now we are doing Fine-Tuning.\n"
      ],
      "metadata": {
        "id": "kJiTFzHA7YjG"
      },
      "id": "kJiTFzHA7YjG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Compile the BERT Model"
      ],
      "metadata": {
        "id": "992XKd4w7sHr"
      },
      "id": "992XKd4w7sHr"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "bsYm0ujG7uhC"
      },
      "id": "bsYm0ujG7uhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "\n",
        "Important Interview Question: **bold text**\n",
        "\n",
        "1.    Why learning rate = 3e-5 ?\n",
        "\n",
        "       Because:\n",
        "\n",
        "           (a.)        BERT already trained\n",
        "\n",
        "           (b.)        Large LR â†’ Destroys learned weights\n",
        "\n",
        "           (c.)        Fine-tuning needs small LR"
      ],
      "metadata": {
        "id": "b4v10ZFv7ydW"
      },
      "id": "b4v10ZFv7ydW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Train the BERT Model"
      ],
      "metadata": {
        "id": "x5qVeLg18DPf"
      },
      "id": "x5qVeLg18DPf"
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "mxAs3Zba8E-i"
      },
      "id": "mxAs3Zba8E-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.   What Happens?\n",
        "\n",
        "     (a.)         Only small weight adjustments\n",
        "\n",
        "     (b.)         Model adapts to sentiment classification"
      ],
      "metadata": {
        "id": "1xLXRe198HJu"
      },
      "id": "1xLXRe198HJu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Evaluate the BERT Model"
      ],
      "metadata": {
        "id": "wcHUnz1F8Y1w"
      },
      "id": "wcHUnz1F8Y1w"
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "nmg9J42n8amO"
      },
      "id": "nmg9J42n8amO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.    Expected Accuracy:\n",
        "\n",
        "      (a.)     ~85â€“90% even with small dataset"
      ],
      "metadata": {
        "id": "ycbI4tdl8cXg"
      },
      "id": "ycbI4tdl8cXg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Plot Accuracy vs Loss"
      ],
      "metadata": {
        "id": "3Gp63vD98xID"
      },
      "id": "3Gp63vD98xID"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uFtUVCjC8zN8"
      },
      "id": "uFtUVCjC8zN8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "1.   What This Shows:\n",
        "\n",
        "     (a.)     If accuracy â†‘ and loss â†“ â†’ Model learning\n",
        "\n",
        "     (b.)     If val loss â†‘ but train loss â†“ â†’ Overfitting"
      ],
      "metadata": {
        "id": "eVgsXrWi83HT"
      },
      "id": "eVgsXrWi83HT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11: Perform Predictions on the sample text"
      ],
      "metadata": {
        "id": "jbKN3cNo9Ive"
      },
      "id": "jbKN3cNo9Ive"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews = [\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"Worst film I have ever seen.\"\n",
        "]\n",
        "\n",
        "sample_encodings = tokenizer(\n",
        "    sample_reviews,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "outputs = model(sample_encodings)\n",
        "logits = outputs.logits\n",
        "\n",
        "predictions = tf.argmax(logits, axis=1).numpy()\n"
      ],
      "metadata": {
        "id": "1u0qDip99XsP"
      },
      "id": "1u0qDip99XsP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Convert Prediction to Label\n",
        "\n",
        "for review, pred in zip(sample_reviews, predictions):\n",
        "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "    print(\"Review:\", review)\n",
        "    print(\"Predicted Sentiment:\", sentiment)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "kdbDGT6A9b5i"
      },
      "id": "kdbDGT6A9b5i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Output:**\n",
        "\n",
        "**Review:**               This movie was absolutely fantastic!\n",
        "\n",
        "**Predicted Sentiment:**   Positive\n",
        "\n",
        "**Review:**                Worst film I have ever seen.\n",
        "\n",
        "**Predicted Sentiment:**    Negative\n"
      ],
      "metadata": {
        "id": "EfRmCWyc9eLg"
      },
      "id": "EfRmCWyc9eLg"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}