{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dd4d1a",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stamming is a text normalization technique used in Natural Language Processing that is used to reduce the word to the root word. The root word may or mat not have a meaning.\n",
    "\n",
    "\n",
    "### Importance of Stemming:\n",
    "\n",
    "1.  It helps in finding out the root word that may or may not have a meaning.\n",
    "\n",
    "2.  It speeds up the NLP related tasks\n",
    "\n",
    "3.  It improves the speed and the accuracy of the model\n",
    "\n",
    "4.  It improves the searching tasks\n",
    "\n",
    "\n",
    "### PorterStemmer\n",
    "\n",
    "Porter Stemmer is a widely used stemming algorithm that is used to reduce the word into the root word by removing the suffixes from it.\n",
    "\n",
    "\n",
    "### Steps used in this Algorithm:-\n",
    "\n",
    "1.  Import all the necessary libraries\n",
    "\n",
    "2.  Include the necessary Resource required for nltk\n",
    "\n",
    "3.  Create a Sample Text\n",
    "\n",
    "4.  Perform the tokenization of  the text\n",
    "\n",
    "5.  Load the English Stopwords\n",
    "\n",
    "6.  Remove all the Stop words from the text\n",
    "\n",
    "7.  Perform stemming on all the filtered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbe1e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'ran', 'studi', 'studi']\n"
     ]
    }
   ],
   "source": [
    "from  nltk.stem  import  PorterStemmer\n",
    "\n",
    "### Create an object for Porter Stemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "## create a list of words\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"studies\", \"studying\"]\n",
    "\n",
    "### perform the stemming over each and every word\n",
    "\n",
    "res = [ps.stem(x) for x in words]\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85a8c9",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  Here \"running\" ----------------> \"run\"  (Root Word has a meaning)\n",
    "\n",
    "2.  Here \"studies\" ----------------> \"studi\" (Root Word has no meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54178e",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "111420a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy  as   np\n",
    "import  pandas as   pd\n",
    "import  matplotlib.pyplot  as  plt\n",
    "import  seaborn            as  sns\n",
    "\n",
    "import  nltk\n",
    "\n",
    "from    nltk.tokenize     import word_tokenize  ,  sent_tokenize\n",
    "\n",
    "from    nltk.stem         import PorterStemmer\n",
    "\n",
    "\n",
    "from    nltk.corpus       import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920c210",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.   numpy  ----------->  Computation of the numerical array\n",
    "\n",
    "2.   pandas ----------->  Data Creation and Manipulation\n",
    "\n",
    "3.   matplotlib ------->  Data Visualization\n",
    "\n",
    "4.   seaborn  --------->  Data Correlation\n",
    "\n",
    "5.   nltk   ----------->  Library for NLP for performing the text preprocessing operations\n",
    "\n",
    "6.   corpus  ---------->  location where the data is stored\n",
    "\n",
    "7.   stopwords -------->  unnecessary words that do not add any meaning to the text\n",
    "\n",
    "8.   tokenize  ---------> breaks the text into samller parts\n",
    "\n",
    "9.   sent_tokenize -----> breaks the paragraphs text into sentences\n",
    "\n",
    "10.  word_tokenize -----> breaks the sentence text into the words\n",
    "\n",
    "11.  PorterStemmer -----> reduces the word into root word by removing the suffixes from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edeefb3",
   "metadata": {},
   "source": [
    "### Step 2: Include the necessary Resource required for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb565c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca47123",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. punkt_tab is an advance library module used in NLTK which contains the pre-trained statistical modules and helps the tokenizer to accurately split the text into the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9b30f",
   "metadata": {},
   "source": [
    "### Step 3: Create a Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d93a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample paragraph\n",
    "text = \"\"\"\n",
    "Natural Language Processing allows computers to understand human language.\n",
    "People are communicating using texts, emails, and social media every day.\n",
    "We are building systems that can automatically analyze and respond intelligently.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8495e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNatural Language Processing allows computers to understand human language.\\nPeople are communicating using texts, emails, and social media every day.\\nWe are building systems that can automatically analyze and respond intelligently.\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64c611",
   "metadata": {},
   "source": [
    "### Step 4: Perform the tokenization of  the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0df7e1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nNatural Language Processing allows computers to understand human language.', 'People are communicating using texts, emails, and social media every day.', 'We are building systems that can automatically analyze and respond intelligently.']\n"
     ]
    }
   ],
   "source": [
    "from  nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "### perform the sentence tokenization\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b79e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'to', 'understand', 'human', 'language', '.', 'People', 'are', 'communicating', 'using', 'texts', ',', 'emails', ',', 'and', 'social', 'media', 'every', 'day', '.', 'We', 'are', 'building', 'systems', 'that', 'can', 'automatically', 'analyze', 'and', 'respond', 'intelligently', '.']\n"
     ]
    }
   ],
   "source": [
    "### perform the word tokenization\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee51c",
   "metadata": {},
   "source": [
    "### Step 5: Load the English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "423dde84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'm', 'my', \"aren't\", 'yourself', \"should've\", 've', 'am', 'herself', 'those', 'couldn', 'does', 'should', 'her', \"hadn't\", 'its', 'hasn', 'down', 'between', 'ma', 'until', 'who', 'him', 'an', 'out', 'wouldn', 'own', 'and', 'be', 'than', \"didn't\", 'i', 'you', \"shouldn't\", 'are', 'mustn', \"you're\", 'most', 'yourselves', 'himself', 'themselves', 're', 'as', 'no', 'before', 'this', 'against', 'further', 'so', 'can', 'won', 'there', 'shan', 'them', 'not', 'ourselves', 'myself', 'from', 'more', 'which', 'if', 'of', 'we', 'did', 'needn', 'after', \"you've\", \"you'll\", 'had', 'do', 's', 'y', 'on', 'again', 'what', 'other', 'have', 'each', \"she's\", 'with', 'few', 'll', 'were', 'under', 'when', 'has', 'off', 'wasn', \"weren't\", \"mightn't\", 'shouldn', 'these', 'mightn', 'a', 'how', 'through', 'their', 'our', 'he', \"don't\", 'didn', 'too', 'some', \"couldn't\", \"that'll\", 'because', 'here', 'for', 'only', 'by', \"shan't\", 'aren', 'why', \"haven't\", 'been', 'but', 'theirs', 'any', \"wasn't\", 'all', 'now', 'hadn', 'during', \"it's\", 'then', 'weren', \"doesn't\", 'me', 'yours', 'don', 't', 'being', 'it', 'they', 'very', 'below', \"needn't\", 'up', \"isn't\", 'where', 'at', 'to', 'such', 'in', 'just', 'was', 'the', 'once', \"hasn't\", 'into', 'is', 'o', 'that', 'will', 'having', 'his', 'ain', 'doing', 'or', 'nor', \"won't\", 'both', 'ours', 'doesn', 'she', 'about', 'over', \"wouldn't\", 'your', \"mustn't\", 'itself', 'whom', 'while', 'same', 'hers', 'd', 'haven', \"you'd\", 'above'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import stopwords\n",
    "\n",
    "\n",
    "### define the set of all the stopwords used in english\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a43d5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'to', 'understand', 'human', 'language', 'People', 'are', 'communicating', 'using', 'texts', 'emails', 'and', 'social', 'media', 'every', 'day', 'We', 'are', 'building', 'systems', 'that', 'can', 'automatically', 'analyze', 'and', 'respond', 'intelligently']\n"
     ]
    }
   ],
   "source": [
    "### perform the regexptokenization on the text\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "### perform the tokenization on the text\n",
    "\n",
    "ans = reg.tokenize(text)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d436b0",
   "metadata": {},
   "source": [
    "### Step 6: Remove all the Stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8fd0f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'understand', 'human', 'language', 'People', 'communicating', 'using', 'texts', 'emails', 'social', 'media', 'every', 'day', 'We', 'building', 'systems', 'automatically', 'analyze', 'respond', 'intelligently']\n"
     ]
    }
   ],
   "source": [
    "res = [x for x in ans if(x not in english_stopwords)]\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa465b4",
   "metadata": {},
   "source": [
    "### Step 7: Perform stemming on all the filtered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d634ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'allow', 'comput', 'understand', 'human', 'languag', 'peopl', 'commun', 'use', 'text', 'email', 'social', 'media', 'everi', 'day', 'we', 'build', 'system', 'automat', 'analyz', 'respond', 'intellig']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "ans = [ps.stem(x) for x in res if(x != ' ')]\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac844e",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  Here all the words has been converetd into their root words that may or may not have a meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
