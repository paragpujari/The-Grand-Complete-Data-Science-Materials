{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6916b5b1",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Stop Words are the most common words that occur frequently in the particular text.\n",
    "\n",
    "It do not add any meaning to the particular text.\n",
    "\n",
    "\n",
    "Ex :- is, and, the , a, an etc.\n",
    "\n",
    "\n",
    "### Importance of Removing the Stop Words :- \n",
    "\n",
    "1. It reduces the noise.\n",
    "\n",
    "2. It is done to improve the performance of the model.\n",
    "\n",
    "3. It is done to imrpove the accuracy of the model.\n",
    "\n",
    "4. It is done to perform the better feature selections of the model.\n",
    "\n",
    "\n",
    "Use Case :-- We need to remove the stop words like a, an, the\n",
    "\n",
    "\n",
    "### To clean a text by removing common stopwords (like “the”, “is”, “and”) using the NLTK library in Python\n",
    "\n",
    "\n",
    "### Steps used in this Algorithm:-\n",
    "\n",
    "1.  Import all the necessary libraries\n",
    "\n",
    "2.  Include the necessary Resource required for nltk\n",
    "\n",
    "3.  Create a Sample Text\n",
    "\n",
    "4.  Perform the tokenization of  the text\n",
    "\n",
    "5.  Load the English Stopwords\n",
    "\n",
    "6.  Remove all the Stop words from the text\n",
    "\n",
    "7.  Perform the Regular Expression Tokenization to remove all the punctuations from the text\n",
    "\n",
    "8.  Write a reusuable function to remove all the stop words from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1ad5b",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "c2a18d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import    numpy               as   np\n",
    "import    pandas              as   pd\n",
    "import    matplotlib.pyplot   as  plt\n",
    "import    seaborn             as  sns\n",
    "\n",
    "\n",
    "import    nltk\n",
    "\n",
    "from      nltk.corpus        import stopwords\n",
    "\n",
    "from      nltk.tokenize      import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100b595",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.   numpy  ----------->  Computation of the numerical array\n",
    "\n",
    "2.   pandas ----------->  Data Creation and Manipulation\n",
    "\n",
    "3.   matplotlib ------->  Data Visualization\n",
    "\n",
    "4.   seaborn  --------->  Data Correlation\n",
    "\n",
    "5.   nltk   ----------->  Library for NLP for performing the text preprocessing operations\n",
    "\n",
    "6.   corpus  ---------->  location where the data is stored\n",
    "\n",
    "7.   stopwords -------->  unnecessary words that do not add any meaning to the text\n",
    "\n",
    "8.   tokenize  ---------> breaks the text into samller parts\n",
    "\n",
    "9.   sent_tokenize -----> breaks the paragraphs text into sentences\n",
    "\n",
    "10.  word_tokenize -----> breaks the sentence text into the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f506f",
   "metadata": {},
   "source": [
    "### Step 2: Include the necessary Resource required for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "cc1b98bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa6468",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. punkt_tab is an advance library module used in NLTK which contains the pre-trained statistical modules and helps the tokenizer to accurately split the text into the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510f71d",
   "metadata": {},
   "source": [
    "### Step 3: Create a Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e803b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural Language Processing (NLP) is a subfield of Artificial Intelligence \n",
    "that helps computers understand human language. It involves text cleaning, \n",
    "tokenization, stopword removal, stemming, and lemmatization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "7f9da44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a subfield of Artificial Intelligence \n",
      "that helps computers understand human language. It involves text cleaning, \n",
      "tokenization, stopword removal, stemming, and lemmatization.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc30574",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Here we have defined the sample text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c71b3",
   "metadata": {},
   "source": [
    "### Step 4:  Perform the tokenization of  the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "a6c4c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Original Text------------------------------\n",
      "Natural Language Processing (NLP) is a subfield of Artificial Intelligence \n",
      "that helps computers understand human language. It involves text cleaning, \n",
      "tokenization, stopword removal, stemming, and lemmatization.\n",
      "--------------------Text after performing the sentence tokenization------------------------------\n",
      "['Natural Language Processing (NLP) is a subfield of Artificial Intelligence \\nthat helps computers understand human language.', 'It involves text cleaning, \\ntokenization, stopword removal, stemming, and lemmatization.']\n",
      "--------------------Text after performing the words tokenization------------------------------\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'Artificial', 'Intelligence', 'that', 'helps', 'computers', 'understand', 'human', 'language', '.', 'It', 'involves', 'text', 'cleaning', ',', 'tokenization', ',', 'stopword', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "--------------------Text after performing the Regexp Tokenization------------------------------\n",
      "Regular Expression Tokens are: ['Natural', 'Language', 'Processing', 'NLP', 'is', 'a', 'subfield', 'of', 'Artificial', 'Intelligence', 'that', 'helps', 'computers', 'understand', 'human', 'language', 'It', 'involves', 'text', 'cleaning', 'tokenization', 'stopword', 'removal', 'stemming', 'and', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize  import  sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "\n",
    "print(\"-------------------Original Text------------------------------\")\n",
    "\n",
    "print(text)\n",
    "\n",
    "\n",
    "print(\"--------------------Text after performing the sentence tokenization------------------------------\")\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "\n",
    "print(\"--------------------Text after performing the words tokenization------------------------------\")\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "\n",
    "print(\"--------------------Text after performing the Regexp Tokenization------------------------------\")\n",
    "\n",
    "\n",
    "### Create an object for Reg exp tokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "### Apply the Reg exp Tokenizer object on the text\n",
    "\n",
    "ans = reg.tokenize(text)\n",
    "\n",
    "print(\"Regular Expression Tokens are:\", ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b16b6",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Tokenization is performed on the sample text in three ways:- \n",
    "\n",
    "  (a.)  sent_tokenize --------->  tokenizes the paragraphs text into sentences\n",
    "\n",
    "  (b.)  word_tokenize --------->  tokenizes the sentences into the words\n",
    "\n",
    "  (c.)  RegexpTokenizer --------> It is the tokenizer that removes all the special characters symbols and returns only numbers and letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08a36e",
   "metadata": {},
   "source": [
    "### Step 5:  Load the English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "175a6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ourselves', 'all', 'hadn', 'off', 'there', \"you've\", 'which', 'between', 'the', 'him', 'not', 'can', 'she', 'just', 'am', 'same', \"shouldn't\", 'so', 'themselves', 'before', 'herself', 'why', 'does', 'to', 'out', 'were', 'has', 'yourself', 'more', 'down', 'because', 'they', 'with', 'ma', 'a', 'under', \"aren't\", \"won't\", 'such', \"mustn't\", 'didn', 'if', 'no', 'isn', 'been', 'when', 'as', 'where', 'again', \"don't\", \"you're\", 'during', \"hadn't\", \"doesn't\", 'doesn', 'then', \"should've\", 'aren', 'and', 'we', 'own', 'o', \"didn't\", \"isn't\", 'shan', 'other', 'my', 'm', 'by', 'himself', 'of', 'won', 'me', 'in', 'here', \"it's\", 'yourselves', 'into', 't', 'further', 'now', \"wasn't\", 'ours', 'on', 'an', 'how', 'those', 'it', 'd', 'hers', 're', 'theirs', 'some', 'through', 'is', 'below', 'most', 'should', 'its', 'up', 'his', 'than', 'mightn', 'for', 'after', 'being', 'haven', 'are', 'weren', 'whom', 'once', 'wouldn', 'shouldn', 'yours', 'don', 'or', 'needn', 'itself', 'nor', 'your', \"mightn't\", 've', 'doing', \"hasn't\", 'until', 'i', 'll', 'very', 'had', 'too', 'couldn', \"you'd\", \"haven't\", \"couldn't\", 'but', 'any', \"she's\", 'our', 'each', 'will', 'be', 'that', \"needn't\", \"wouldn't\", 'he', 'them', 'few', 'only', 'did', 'against', 'have', 'while', \"shan't\", \"you'll\", 'about', 'both', 'ain', 'their', 'y', 'over', 'myself', 'these', 'having', 'hasn', 'mustn', 's', 'was', \"that'll\", 'what', 'above', \"weren't\", 'wasn', 'this', 'her', 'who', 'from', 'you', 'at', 'do'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import stopwords\n",
    "\n",
    "\n",
    "### define the english stop words\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef972075",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Here we have  imported the module of stopwords from the corpus text\n",
    "\n",
    "2. Then we have defined all the unique stopwords of the english language.\n",
    "\n",
    "3. Then we have printed all the stop words of english language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192b2fa",
   "metadata": {},
   "source": [
    "### Step 6:  Remove all the Stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4e0ca57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'NLP', 'subfield', 'Artificial', 'Intelligence', 'helps', 'computers', 'understand', 'human', 'language', 'It', 'involves', 'text', 'cleaning', 'tokenization', 'stopword', 'removal', 'stemming', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "### define the empty list for storing all the filtered words\n",
    "\n",
    "filtered_list = []\n",
    "\n",
    "### Store all the filtered words and remove all the stop words from the text\n",
    "\n",
    "for x in ans:\n",
    "    ### Consider all those words do not exists in the english stop words\n",
    "    if(x not in english_stopwords):\n",
    "        ### Store all the filtered words in the filtered  list\n",
    "        filtered_list.append(x)\n",
    "        \n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "4857cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert all the words in the lists to text\n",
    "\n",
    "filtered_lists = '  '.join(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "613c3a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural  Language  Processing  NLP  subfield  Artificial  Intelligence  helps  computers  understand  human  language  It  involves  text  cleaning  tokenization  stopword  removal  stemming  lemmatization'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd6bb1",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Here we have defined an empty list named (filtered_list) that contains all the filtered words which are not the stop words.\n",
    "\n",
    "2. First of all, we have  looped through all the elements in the lists obtained from Regular Expression.\n",
    "\n",
    "3. Then we have check whether every filtered regular expression word exists in the english stop word set or not. If exists then we are not including in the filtered_list.\n",
    "\n",
    "4. Then We are converting the words in the list to texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a201801",
   "metadata": {},
   "source": [
    "### Step 7: Perform the Regular Expression Tokenization to remove all the punctuations from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "0362b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'Artificial', 'Intelligence', 'that', 'helps', 'computers', 'understand', 'human', 'language', '.', 'It', 'involves', 'text', 'cleaning', ',', 'tokenization', ',', 'stopword', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "Natural Language Processing ( NLP ) is a subfield of Artificial Intelligence that helps computers understand human language . It involves text cleaning , tokenization , stopword removal , stemming , and lemmatization .\n",
      "Tokenized words after performing teh Regular Expression Tokenizer is: Natural Language Processing NLP is a subfield of Artificial Intelligence that helps computers understand human language It involves text cleaning tokenization stopword removal stemming and lemmatization\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize  import   word_tokenize\n",
    "\n",
    "\n",
    "### perform the word tokenization on the texts\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "### Convert the tokenized words in the list into strings\n",
    "\n",
    "words = ' '.join(words)\n",
    "\n",
    "print(words)\n",
    "\n",
    "### Use The RegExpTokenizer to remove all the punctuations from the text\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "ans = reg.tokenize(words)\n",
    "\n",
    "ans = ' '.join(ans)\n",
    "\n",
    "print(\"Tokenized words after performing teh Regular Expression Tokenizer is:\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634b4e8",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  Performed the word tokenization on the text\n",
    "\n",
    "2.  Converted the tokenized words in the list to strings\n",
    "\n",
    "3.  Performed the Regular Expression Tokenization on the tokenized words to remove all the punctuations from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa79e6",
   "metadata": {},
   "source": [
    "### Step 8:  Write a reusuable function to remove all the stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "123678e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This simple NLP example showing remove stopwords text .\n"
     ]
    }
   ],
   "source": [
    "### define the function\n",
    "def remove_stopwords(text):\n",
    "    ### perform the word tokenization on the text\n",
    "    ans = word_tokenize(text)\n",
    "    ### Check if the tokenized word is not the part of the english stopwords then include it in the result\n",
    "    res = [x for x in ans if(x not in english_stopwords)]\n",
    "    ### Convert all the filtered words in the list to words\n",
    "    res = ' '.join(res)\n",
    "    return(res)\n",
    "\n",
    "\n",
    "### call the function for removing all the stop words from the text\n",
    "sample = 'This is a simple NLP example showing how to remove stopwords from text.'\n",
    "ans = remove_stopwords(sample)\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
