{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a79f69",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the first step used in Natural Language Processing that is used to break the entire text corpus into smaller parts known as tokens.\n",
    "\n",
    "There are two types of Tokenization:- \n",
    "\n",
    "1.  Sentence Tokenization --------->  It is the process of breaking the paragraphs into the sentences.\n",
    "\n",
    "Ex:-- Input :- \"I love Data Science. It is amazing!\"\n",
    "\n",
    "      Output :- [\"I love Data Science.\",\"It is amazing!\"]\n",
    "\n",
    "2.  Word     Tokenization --------->  It is the process of breaking the sentences into words.\n",
    "\n",
    "Ex:-- Input :- \"I love Data Science.\"\n",
    "\n",
    "      Output :- [\"I\",\"love\",\"Data\",\"Science.\"]\n",
    "\n",
    "\n",
    "### Steps used in this Algorithm:-\n",
    "\n",
    "1.  Import all the necessary libraries\n",
    "\n",
    "2.  Include the necessary Resource required for nltk\n",
    "\n",
    "3.  Create a Sample Text Dataset\n",
    "\n",
    "4.  Perform the Sentence Tokenization\n",
    "\n",
    "5.  Perform the Word Tokenization\n",
    "\n",
    "6.  Perform the RegExpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5aef81",
   "metadata": {},
   "source": [
    "### Step 1:  Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "532f5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy              as   np\n",
    "import  pandas             as   pd\n",
    "import  matplotlib.pyplot  as  plt\n",
    "import  seaborn            as  sns\n",
    "\n",
    "import  nltk\n",
    "\n",
    "from    nltk.tokenize    import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0c616",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  numpy ------------------>  Computation of the numerical array\n",
    "\n",
    "2.  pandas ----------------->  Data Creation and Manipulation\n",
    "\n",
    "3.  matplotlib ------------->  Data Visualization\n",
    "\n",
    "4.  seaborn   -------------->  Data Correlation\n",
    "\n",
    "5.  nltk    ----------------> Plays an imporatnt role in text preprocessing\n",
    "\n",
    "6.  tokenize ---------------> Breaks the text into the smaller parts\n",
    "\n",
    "7.  sent_tokenize ----------> Breaks the paragraphs into sentences\n",
    "\n",
    "8.  word_tokenize ----------> Breaks the sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df56ce",
   "metadata": {},
   "source": [
    "### Step 2: Include the necessary Resource required for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21a94ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6983cc",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  punkt_tab is an advance library module used in NLTK that contains the pre-trained statistical tables and helps the tokenizer to accurately split the text into the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58437679",
   "metadata": {},
   "source": [
    "### Step 3: Create a Sample Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff9536aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Tokenization is the first step in text processing. \n",
    "It breaks the text into smaller parts called tokens — words, phrases, or symbols.\n",
    "This helps machines understand human language more effectively!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "92e29e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the first step in text processing. \n",
      "It breaks the text into smaller parts called tokens — words, phrases, or symbols.\n",
      "This helps machines understand human language more effectively!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4ae88",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Here a sample text data is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7cec4",
   "metadata": {},
   "source": [
    "### Step 4: Perform the Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b342f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Printing the Original text----------------------------------------------\n",
      "Tokenization is the first step in text processing. \n",
      "It breaks the text into smaller parts called tokens — words, phrases, or symbols.\n",
      "This helps machines understand human language more effectively!\n",
      "\n",
      "-----------------Performing Sentence tokenization on the paragraphs texts-----------------\n",
      "['Tokenization is the first step in text processing.', 'It breaks the text into smaller parts called tokens — words, phrases, or symbols.', 'This helps machines understand human language more effectively!']\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------Printing the Original text----------------------------------------------\")\n",
    "\n",
    "print(text)\n",
    "\n",
    "print(\"-----------------Performing Sentence tokenization on the paragraphs texts-----------------\")\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809db43",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. We have printed the original text.\n",
    "\n",
    "2. Setence tokenization is performed on the entire sample text where the entire text is divided into the sentences.\n",
    "\n",
    "3. We have obtained the output in the form of the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbba5d",
   "metadata": {},
   "source": [
    "### Step 5: Perform the Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fbbace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Original Sentences in texts---------------------------------------------\n",
      "Tokenization is the first step in text processing. It breaks the text into smaller parts called tokens — words, phrases, or symbols. This helps machines understand human language more effectively!\n",
      "-----------------------------Word Tokenization-------------------------------------------------------\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'processing', '.', 'It', 'breaks', 'the', 'text', 'into', 'smaller', 'parts', 'called', 'tokens', '—', 'words', ',', 'phrases', ',', 'or', 'symbols', '.', 'This', 'helps', 'machines', 'understand', 'human', 'language', 'more', 'effectively', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "### Join all the sentences in the lists into the texts\n",
    "\n",
    "sent = ' '.join(sentences)\n",
    "\n",
    "\n",
    "print(\"-----------------------------Original Sentences in texts---------------------------------------------\")\n",
    "\n",
    "print(sent)\n",
    "\n",
    "print(\"-----------------------------Word Tokenization-------------------------------------------------------\")\n",
    "\n",
    "ans = word_tokenize(sent)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb30225",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. We have joined all the sentences in the lists into the sentences. Now we have the original sentence text.\n",
    "\n",
    "2. Word Tokenization is performed over the sentences and the sentence text is divided into the words.\n",
    "\n",
    "3. We have obtained the output in the form of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882543d3",
   "metadata": {},
   "source": [
    "### Step 6: Perform the RegExpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a7ca831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Printing the Original text----------------------------------------------\n",
      "Tokenization is the first step in text processing. \n",
      "It breaks the text into smaller parts called tokens — words, phrases, or symbols.\n",
      "This helps machines understand human language more effectively!\n",
      "\n",
      "-----------------Printing the Cleaned text after the tokenization of the regular expression text---------------------------------------------\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'processing', 'It', 'breaks', 'the', 'text', 'into', 'smaller', 'parts', 'called', 'tokens', 'words', 'phrases', 'or', 'symbols', 'This', 'helps', 'machines', 'understand', 'human', 'language', 'more', 'effectively']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize  import RegexpTokenizer\n",
    "\n",
    "### Create an object for Regular Expression Tokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "print(\"-----------------Printing the Original text----------------------------------------------\")\n",
    "\n",
    "print(text)\n",
    "\n",
    "print(\"-----------------Printing the Cleaned text after the tokenization of the regular expression text---------------------------------------------\")\n",
    "### Tokenize the custom regular expression\n",
    "\n",
    "print(reg.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50f1c5",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. An object for Regular expression Tokenizer has been defined.\n",
    "\n",
    "   (i)  \\w ---------> extracts only numbers and letters\n",
    "\n",
    "   (ii) +  ---------> Comprises of one or more occurances\n",
    "\n",
    "\n",
    "2. Here we have a sample text\n",
    "\n",
    "3. Regular Expression Tokenizer has been applied on the text where it removes all ',','.','$' and extracts only numbers and letters.\n",
    "\n",
    "4.  Output is in the form of words enclosed in lists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
