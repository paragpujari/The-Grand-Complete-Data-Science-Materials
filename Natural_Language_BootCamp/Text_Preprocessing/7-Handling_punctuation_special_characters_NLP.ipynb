{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7704c984",
   "metadata": {},
   "source": [
    "### Handling punctuation & special characters \n",
    "\n",
    "\n",
    "### simple end-to-end Python implementation of Handling punctuation & special characters  using NLTK  \n",
    "\n",
    "\n",
    "#### Step used in this Algorithm:---\n",
    "\n",
    "1.   Import all the necessary libraries\n",
    "\n",
    "2.   Download all the NLTK resources\n",
    "\n",
    "3.   Define the Sample Text\n",
    "\n",
    "4.   Tokenize the text\n",
    "\n",
    "5.   Define the english stop words and remove the stop words from the text\n",
    "\n",
    "6.   Use the Regular Expression Tokenization and remove all the punctuations\n",
    "\n",
    "7.   Perform POS Tagging on the refined words in the text\n",
    "\n",
    "8.   Perform Lemmatization as well as POS Tagging on the refined words in the text\n",
    "\n",
    "9.   Perform the Named Entity Recognition (NER) on the POS words in the text and plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0259a3",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b7967fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import   numpy              as   np\n",
    "import   pandas             as   pd\n",
    "import   matplotlib.pyplot  as  plt\n",
    "import   seaborn            as  sns\n",
    "\n",
    "\n",
    "import   nltk\n",
    "\n",
    "\n",
    "from     nltk.tokenize     import  word_tokenize, sent_tokenize\n",
    "from     nltk.corpus       import  stopwords\n",
    "from     nltk.stem         import  WordNetLemmatizer\n",
    "from     nltk              import  pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20859854",
   "metadata": {},
   "source": [
    "### Step 2:  Download all the NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "62b335b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger_eng: Package\n",
      "[nltk_data]     'average_perceptron_tagger_eng' not found in index\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('average_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140b0d4",
   "metadata": {},
   "source": [
    "### Step 3:  Define the Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "124df1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Hello!!! My name is Parag Pujari :) #DataScience @2026 $$$ Welcome to NLP--world!!!\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing punctuation and special characters\n",
    "text = \"Hello!!! My name is Parag Pujari :) #DataScience @2026 $$$ Welcome to NLP--world!!!\"\n",
    "print(\"Original Text:\\n\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b7907d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!!! My name is Parag Pujari :) #DataScience @2026 $$$ Welcome to NLP--world!!!'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee517b",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.   This text contains:\n",
    "\n",
    "        (a.)   Multiple punctuation marks (!!!, --)\n",
    "\n",
    "        (b.)   Special characters (#, @, $)\n",
    "\n",
    "        (c.)   Emoticons (:)\n",
    "\n",
    "        (d.)  Numbers (2026)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297f969",
   "metadata": {},
   "source": [
    "### Step 4: Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b98c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', '!', '!', 'My', 'name', 'is', 'Parag', 'Pujari', ':', ')', '#', 'DataScience', '@', '2026', '$', '$', '$', 'Welcome', 'to', 'NLP', '--', 'world', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a6ba1",
   "metadata": {},
   "source": [
    "### Step 5:  Define the english stop words and remove the stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5bbdbf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "['Hello', '!', '!', '!', 'My', 'name', 'Parag', 'Pujari', ':', ')', '#', 'DataScience', '@', '2026', '$', '$', '$', 'Welcome', 'NLP', '--', 'world', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "englishwords = stopwords.words(\"english\")\n",
    "print(englishwords)\n",
    "\n",
    "### Remove all the englis stopwords from the text\n",
    "\n",
    "res = [x for x in words if(x not in englishwords)]\n",
    "\n",
    "print(res)\n",
    "\n",
    "res = \" \".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423a9df",
   "metadata": {},
   "source": [
    "### Step 6: Use the Regular Expression Tokenization and remove all the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9d0133a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'My', 'name', 'Parag', 'Pujari', 'DataScience', '2026', 'Welcome', 'NLP', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "resdata = reg.tokenize(res)\n",
    "\n",
    "print(resdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20eeeb",
   "metadata": {},
   "source": [
    "### Step 7: Perform POS Tagging on the refined words in the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4aac2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('My', 'NNP'), ('name', 'NN'), ('Parag', 'NNP'), ('Pujari', 'NNP'), ('DataScience', 'NNP'), ('2026', 'CD'), ('Welcome', 'NNP'), ('NLP', 'NNP'), ('world', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "postagdata = pos_tag(resdata)\n",
    "\n",
    "print(postagdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf791ba",
   "metadata": {},
   "source": [
    "### Step 8: Perform Lemmatization as well as POS Tagging on the refined words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2ad04021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1a964efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'My', 'name', 'Parag', 'Pujari', 'DataScience', '2026', 'Welcome', 'NLP', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem  import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from  nltk.corpus import wordnet\n",
    "\n",
    "### Create an empty list to store all the lemmatized words\n",
    "\n",
    "lemmatized_words = []\n",
    "\n",
    "for word, pos in postagdata:\n",
    "    ### get the pos tag\n",
    "    poss = get_wordnet_pos(pos)\n",
    "    ### perform the lemmatization on the text\n",
    "    lemdata = lemma.lemmatize(word, pos = poss)\n",
    "    lemmatized_words.append(lemdata)\n",
    "\n",
    "\n",
    "print(lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c88667",
   "metadata": {},
   "source": [
    "### Step 9:  Perform the Named Entity Recognition (NER) on the POS words in the text and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "241f1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "nechunkdata = ne_chunk(postagdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "31a48618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,592.0,168.0\" width=\"592px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"10.8108%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Hello</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.40541%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.75676%\" x=\"10.8108%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">My</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.1892%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.10811%\" x=\"17.5676%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">name</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.6216%\" y1=\"20px\" y2=\"48px\" /><svg width=\"20.2703%\" x=\"25.6757%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"46.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Parag</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.3333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"53.3333%\" x=\"46.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Pujari</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.3333%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.8108%\" y1=\"20px\" y2=\"48px\" /><svg width=\"17.5676%\" x=\"45.9459%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DataScience</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.7297%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.10811%\" x=\"63.5135%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">2026</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.5676%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.1622%\" x=\"71.6216%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Welcome</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.7027%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.75676%\" x=\"83.7838%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NLP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.1622%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.45946%\" x=\"90.5405%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">world</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.2703%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Hello', 'NNP')]), ('My', 'NNP'), ('name', 'NN'), Tree('PERSON', [('Parag', 'NNP'), ('Pujari', 'NNP')]), ('DataScience', 'NNP'), ('2026', 'CD'), ('Welcome', 'NNP'), ('NLP', 'NNP'), ('world', 'NN')])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nechunkdata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
