{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ad32d9",
   "metadata": {},
   "source": [
    "### Lemmatization:\n",
    "\n",
    "Lemmatization is a process of converting the word into the root word or the base word that has a valid meaning.\n",
    "\n",
    "### Importance of Lemmatization:\n",
    "\n",
    "1. It reduces the word to the base word that has a valid meaning\n",
    "\n",
    "2. It is used for Information Retrival\n",
    "\n",
    "3. It is used for topic modelling\n",
    "\n",
    "4.  It is used for text analysis\n",
    "\n",
    "5. It is used for text classification.\n",
    "\n",
    "### WordNetLemmatizer():\n",
    "\n",
    "WordNetLemmatizer is a kind of lemmatizer used in NLTK library.\n",
    "\n",
    "It has a word net lexical database that is used to find out the root form of the word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "95a13f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "### Find out thw root form of the word 'running' in verb mode\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "### create the object form of the WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "### Find out the root form of the word 'running' in verb form\n",
    "\n",
    "ans = lemma.lemmatize('running', pos='v')\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2d875b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "### Find out the root form of the word 'running' in adjective form\n",
    "\n",
    "ans = lemma.lemmatize('running', pos='a')\n",
    "\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6f7b408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "### Find out the root form of the word 'running' in noun form\n",
    "\n",
    "ans = lemma.lemmatize('running', pos = 'n')\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b5cc7",
   "metadata": {},
   "source": [
    "### Perform the end to end text preprocessing using Lemmatization\n",
    "\n",
    "\n",
    "#### Steps used in this Algorithm:-\n",
    "\n",
    "1.   Import all the necessary libraries\n",
    "\n",
    "2.   Include the necessary Resource required for nltk\n",
    "\n",
    "3.   Define the Sample Text\n",
    "\n",
    "4.   Perform word tokenization on the Sample Text\n",
    "\n",
    "5.   Define the english stop words and remove it\n",
    "\n",
    "6.   Define the Regular Expressions to remove all the punctautions from the text\n",
    "\n",
    "7.   Peforming Lemmatization on the filtered words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58e5ba",
   "metadata": {},
   "source": [
    "### Step 1:  Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a6b63857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import   numpy               as   np\n",
    "import   pandas              as   pd\n",
    "import   matplotlib.pyplot   as   plt\n",
    "import   seaborn             as   sns\n",
    "\n",
    "import   nltk\n",
    "\n",
    "from     nltk.tokenize      import  word_tokenize, sent_tokenize\n",
    "from     nltk.corpus        import  stopwords\n",
    "from     nltk.stem          import  WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0684e",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.   numpy  ----------->  Computation of the numerical array\n",
    "\n",
    "2.   pandas ----------->  Data Creation and Manipulation\n",
    "\n",
    "3.   matplotlib ------->  Data Visualization\n",
    "\n",
    "4.   seaborn  --------->  Data Correlation\n",
    "\n",
    "5.   nltk   ----------->  Library for NLP for performing the text preprocessing operations\n",
    "\n",
    "6.   corpus  ---------->  location where the data is stored\n",
    "\n",
    "7.   stopwords -------->  unnecessary words that do not add any meaning to the text\n",
    "\n",
    "8.   tokenize  ---------> breaks the text into samller parts\n",
    "\n",
    "9.   sent_tokenize -----> breaks the paragraphs text into sentences\n",
    "\n",
    "10.  word_tokenize -----> breaks the sentence text into the words\n",
    "\n",
    "11.  WordNetLemmatizer -----> It has a Word Net lexical database that reduces the word to the root word which has a meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b3f5d",
   "metadata": {},
   "source": [
    "### Step 2: Include the necessary Resource required for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b3f88dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155722b",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  'punkt_tab' is a pre trained statistical tables that helps the tokenizer to accurately split the text into the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed493f9",
   "metadata": {},
   "source": [
    "### Step 3:  Define the Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d4222709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample paragraph\n",
    "text = \"\"\"\n",
    "Natural Language Processing allows computers to understand human language.\n",
    "People are communicating using texts, emails, and social media every day.\n",
    "We are building systems that can automatically analyze and respond intelligently.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f6376855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNatural Language Processing allows computers to understand human language.\\nPeople are communicating using texts, emails, and social media every day.\\nWe are building systems that can automatically analyze and respond intelligently.\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280bcd5",
   "metadata": {},
   "source": [
    "### Step 4:  Perform word tokenization on the Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "27f58142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'to', 'understand', 'human', 'language', '.', 'People', 'are', 'communicating', 'using', 'texts', ',', 'emails', ',', 'and', 'social', 'media', 'every', 'day', '.', 'We', 'are', 'building', 'systems', 'that', 'can', 'automatically', 'analyze', 'and', 'respond', 'intelligently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize  import word_tokenize\n",
    "\n",
    "\n",
    "### perform the word tokenization on the texts\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef70584",
   "metadata": {},
   "source": [
    "### Step 5:  Define the english stop words and remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f3e669ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------List of all the english stop words----------------------------------------------\n",
      "{'has', 's', 'to', \"that'll\", 'himself', 'nor', 'too', 'can', \"don't\", 'it', \"aren't\", 'yours', 'down', 'here', 'most', \"hasn't\", \"you'll\", 'few', \"she's\", 'some', 'his', 'do', 'll', 'but', 'yourself', 'which', 'as', 'under', 'about', 'ain', 'so', 'off', 'does', 'not', 'of', 'on', \"it's\", 'isn', 'yourselves', 'those', \"wouldn't\", 'their', 'themselves', 'a', 'having', 'ours', 'all', 'very', 'doesn', 'needn', 'is', \"wasn't\", 'when', 'the', \"hadn't\", \"isn't\", \"didn't\", 'm', 'now', 'aren', 'your', 'where', 'shouldn', 'there', 'and', 'only', 'had', 'ma', 'such', 'our', \"you've\", 'any', 'just', 'out', 'no', 'you', 'were', 'herself', 'wasn', 'or', \"you're\", 'am', 'hers', 'then', 'more', \"doesn't\", \"haven't\", 'between', 'myself', \"needn't\", 'don', 'o', 'we', 'him', 'how', 'why', 'than', 'what', 'that', \"mightn't\", 'before', 'both', 'will', 'if', 'against', 'mustn', 'was', \"you'd\", 'above', 'won', 've', 'each', 'who', 'up', 'haven', 'theirs', 'ourselves', 'they', 'my', 'further', \"mustn't\", 'by', 'should', 'for', 'be', 'with', \"shan't\", 'been', 't', 'couldn', 'have', 'through', 'doing', 'other', 'itself', 're', 'are', 'this', 'being', \"couldn't\", 'over', 'd', 'own', 'weren', 'from', 'into', 'an', 'until', 'while', 'hasn', \"won't\", 'me', 'its', 'y', 'mightn', 'these', 'i', 'after', 'at', 'because', 'didn', 'whom', 'he', 'them', 'again', 'below', 'she', 'wouldn', \"shouldn't\", 'shan', 'during', 'once', \"weren't\", 'her', 'same', \"should've\", 'did', 'hadn', 'in'}\n",
      "--------------------------Filtered words-----------------------------------------------------------------\n",
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'understand', 'human', 'language', '.', 'People', 'communicating', 'using', 'texts', ',', 'emails', ',', 'social', 'media', 'every', 'day', '.', 'We', 'building', 'systems', 'automatically', 'analyze', 'respond', 'intelligently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "### define the set of english stopwords\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(\"--------------------------List of all the english stop words----------------------------------------------\")\n",
    "\n",
    "print(english_stopwords)\n",
    "\n",
    "\n",
    "### Remove the stop words from the text\n",
    "\n",
    "res = [x for x in words if(x not in english_stopwords)]\n",
    "\n",
    "print(\"--------------------------Filtered words-----------------------------------------------------------------\")\n",
    "print(res)\n",
    "\n",
    "\n",
    "### Convert the rsult from the list to string\n",
    "\n",
    "res = \" \".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f9de5",
   "metadata": {},
   "source": [
    "### Step 6: Define the Regular Expressions to remove all the punctautions from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "03926771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'understand', 'human', 'language', 'People', 'communicating', 'using', 'texts', 'emails', 'social', 'media', 'every', 'day', 'We', 'building', 'systems', 'automatically', 'analyze', 'respond', 'intelligently']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize  import RegexpTokenizer\n",
    "\n",
    "\n",
    "### Create the object for Regular Expression Tokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "### perform the Regular Expression Tokenizer to remove all the punctuations from the text\n",
    "\n",
    "ans = reg.tokenize(res)   ### Regular Expression Tokenizer accepts the input in the form of string\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606ddb3",
   "metadata": {},
   "source": [
    "### Step 7: Peforming Lemmatization on the filtered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "95b262ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Original Text-------------------------------------------------------------------\n",
      "['Natural', 'Language', 'Processing', 'allows', 'computers', 'understand', 'human', 'language', 'People', 'communicating', 'using', 'texts', 'emails', 'social', 'media', 'every', 'day', 'We', 'building', 'systems', 'automatically', 'analyze', 'respond', 'intelligently']\n",
      "------------------------Lemmatized Text-------------------------------------------------------------------\n",
      "['natural', 'language', 'processing', 'allows', 'computer', 'understand', 'human', 'language', 'people', 'communicating', 'using', 'text', 'email', 'social', 'medium', 'every', 'day', 'we', 'building', 'system', 'automatically', 'analyze', 'respond', 'intelligently']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem  import  WordNetLemmatizer\n",
    "\n",
    "\n",
    "### Create an object for Lemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "### Convert the result from string to list\n",
    "\n",
    "ans = list(ans)\n",
    "\n",
    "print(\"------------------------Original Text-------------------------------------------------------------------\")\n",
    "print(ans)\n",
    "\n",
    "### perform the lemmatization on the filtered list\n",
    "\n",
    "res = [lemma.lemmatize(x.lower()) for x in ans]\n",
    "\n",
    "print(\"------------------------Lemmatized Text-------------------------------------------------------------------\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497171ce",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Lemmatization is applied on all the filtered words and now we have all the root form of the words which has a valid meaning in the dictionary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
