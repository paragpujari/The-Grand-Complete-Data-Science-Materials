{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bf9ea4",
   "metadata": {},
   "source": [
    "### Word-Embeddings:\n",
    "\n",
    "Word-Embeddings are the  numerical dense valued representations of the words in the text. It helps in finding out the similarity between two words in the text and the similar words have the same dense vector values.\n",
    "\n",
    "It helps in finding out the relation between the words in the text.\n",
    "\n",
    "It captures the similarity between the two words in the text in the form of vectors.\n",
    "\n",
    "\n",
    "### Importances of Word-Embeddings:---\n",
    "\n",
    "1. It is used to find out the similarity score between two word in the text in the form of dense dimensional vectors.\n",
    "\n",
    "2. It is used to find out the meaning behind every text.\n",
    "\n",
    "3. It is useful for dimensionality reduction.\n",
    "\n",
    "4. It is used to improve the accuracy and the performance of the model\n",
    "\n",
    "5. It helps in finding out the relation between the words in th etext and even finds out which words are more similar. It is also represented in the form of similarity score dense dimensional vectors.\n",
    "\n",
    "\n",
    "\n",
    "Ex:---\n",
    "\n",
    "1.   king - man + woman = queen\n",
    "\n",
    "\n",
    "### Write a simple end to end implementation based on Word Embeddings based on Skip-Grams\n",
    "\n",
    "\n",
    "#### Steps used in this Algorithm :----\n",
    "\n",
    "1.   Import all the necessary libraries\n",
    "\n",
    "2.   Download all the necessary libraries for NLTK\n",
    "\n",
    "3.   Define the corpus text\n",
    "\n",
    "4.   Perform the normalization of the text\n",
    "\n",
    "5.   Perform the Word tokenization on the text\n",
    "\n",
    "6.   Train the Word2Vec Model based on Skip-Grams\n",
    "\n",
    "7.   Explore Word Vectors\n",
    "\n",
    "8.   Find out the vector representation of the words that are most similar to the words 'and' and 'machine'\n",
    "\n",
    "9.   To show the list of all the words that the model has learnt for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80484551",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e91e7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nltk  \n",
    "\n",
    "from    nltk.tokenize  import   word_tokenize\n",
    "\n",
    "from    gensim.models  import   Word2Vec\n",
    "\n",
    "import  numpy   as   np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e90391",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  nltk ------------->   Library used for text preprocessing\n",
    "\n",
    "2.  tokenize --------->   used for breaking the text into smaller parts\n",
    "\n",
    "3.  word_tokenize ---->   breaks the sentences into words\n",
    "\n",
    "4.  gensim ----------->   model used for embedding\n",
    "\n",
    "5.  Word2Vec -----------> converts each word into numerical dense dimensional vector and depicts the relationship between them\n",
    "\n",
    "6.  numpy ------------->  Computation of numerical array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ff3a1",
   "metadata": {},
   "source": [
    "### Step 2:  Download all the necessary libraries for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "acae34de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger_eng: Package\n",
      "[nltk_data]     'average_perceptron_tagger_eng' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk.download('average_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4caf1f5",
   "metadata": {},
   "source": [
    "### OBSEREVATIONS:\n",
    "\n",
    "1.  Here we are downloading the necessary libraries needed for NLTK.\n",
    "\n",
    "    (a.)   punkt_tab ------------>  Tokenization model\n",
    "\n",
    "    (b.)   average_perceptron_tagger_eng ------------>  POS Tagging Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b2c1f",
   "metadata": {},
   "source": [
    "### Step 3: Define the corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "08b26296",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing and Machine Learning are closely related fields.\",\n",
    "    \"Word embeddings are a type of word representation that allows words to be represented as vectors.\",\n",
    "    \"Deep learning models are used for creating high-quality word embeddings.\",\n",
    "    \"The meaning of words can be captured by training on large text datasets.\",\n",
    "    \"Gensim is a popular library for training Word2Vec models in Python.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a0573f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing and Machine Learning are closely related fields.',\n",
       " 'Word embeddings are a type of word representation that allows words to be represented as vectors.',\n",
       " 'Deep learning models are used for creating high-quality word embeddings.',\n",
       " 'The meaning of words can be captured by training on large text datasets.',\n",
       " 'Gensim is a popular library for training Word2Vec models in Python.']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f7429",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  The corpus is the dataset that contains the set of sentences used for text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e4041a",
   "metadata": {},
   "source": [
    "### Step 4:  Perform the normalization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c4a7cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'and', 'Machine', 'Learning', 'are', 'closely', 'related', 'fields', 'Word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'to', 'be', 'represented', 'as', 'vectors', 'Deep', 'learning', 'models', 'are', 'used', 'for', 'creating', 'high', 'quality', 'word', 'embeddings', 'The', 'meaning', 'of', 'words', 'can', 'be', 'captured', 'by', 'training', 'on', 'large', 'text', 'datasets', 'Gensim', 'is', 'a', 'popular', 'library', 'for', 'training', 'Word2Vec', 'models', 'in', 'Python']\n",
      "Natural Language Processing and Machine Learning are closely related fields Word embeddings are a type of word representation that allows words to be represented as vectors Deep learning models are used for creating high quality word embeddings The meaning of words can be captured by training on large text datasets Gensim is a popular library for training Word2Vec models in Python\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "### Create an object for RegexpTokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Accept the input in the form of string\n",
    "\n",
    "corpus = \" \".join(corpus)\n",
    "\n",
    "### using the object for Regular Expression Tokenizer, normalize the text\n",
    "\n",
    "res = reg.tokenize(corpus)\n",
    "\n",
    "print(res)\n",
    "\n",
    "### Again convert the list to strings\n",
    "\n",
    "res = \" \".join(res)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6084b",
   "metadata": {},
   "source": [
    "### Step 5:  Perform the Word tokenization on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cba08c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'and', 'Machine', 'Learning', 'are', 'closely', 'related', 'fields', 'Word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'to', 'be', 'represented', 'as', 'vectors', 'Deep', 'learning', 'models', 'are', 'used', 'for', 'creating', 'high', 'quality', 'word', 'embeddings', 'The', 'meaning', 'of', 'words', 'can', 'be', 'captured', 'by', 'training', 'on', 'large', 'text', 'datasets', 'Gensim', 'is', 'a', 'popular', 'library', 'for', 'training', 'Word2Vec', 'models', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(res)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466ef97",
   "metadata": {},
   "source": [
    "### Step 6: Train the Word2Vec Model based on Skip-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "21f3c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural', 'language', 'processing', 'and', 'machine', 'learning', 'are', 'closely', 'related', 'fields', '.'], ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'to', 'be', 'represented', 'as', 'vectors', '.'], ['deep', 'learning', 'models', 'are', 'used', 'for', 'creating', 'high-quality', 'word', 'embeddings', '.'], ['the', 'meaning', 'of', 'words', 'can', 'be', 'captured', 'by', 'training', 'on', 'large', 'text', 'datasets', '.'], ['gensim', 'is', 'a', 'popular', 'library', 'for', 'training', 'word2vec', 'models', 'in', 'python', '.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing and Machine Learning are closely related fields.\",\n",
    "    \"Word embeddings are a type of word representation that allows words to be represented as vectors.\",\n",
    "    \"Deep learning models are used for creating high-quality word embeddings.\",\n",
    "    \"The meaning of words can be captured by training on large text datasets.\",\n",
    "    \"Gensim is a popular library for training Word2Vec models in Python.\"\n",
    "]\n",
    "\n",
    "tokenized_sentences = [word_tokenize(x.lower()) for x in corpus]\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fcf771a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the Word2Vec Model using Skip-Grams\n",
    "\n",
    "skipgrams_model = Word2Vec(\n",
    "    sentences        =             tokenized_sentences           ,\n",
    "    vector_size      =                50                         ,\n",
    "    window           =                 3                         ,\n",
    "    min_count        =                 1                         ,\n",
    "    sg               =                 1                         ,\n",
    "    epochs           =              100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "72451788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x18866a3c400>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ea151",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The skip-grams model has been trained with the help of the following input parameters:---\n",
    "\n",
    "   (a.)    sentences -------> tokenized_sentences . It contains all the sentences in the form of list of sentences with tokens\n",
    "\n",
    "   (b.)    vector_size = 50 ----------> Each and every input word is converted into numerical dense dimensional vector of size 50\n",
    "\n",
    "   (c.)    window = 3     ------------> Total inputs for context input is 3 needed for embedding.\n",
    "\n",
    "   (d.)    min_count = 1 -------------> combine all the context input whose frequency is less than 1.\n",
    "\n",
    "   (e.)    sg = 1      ---------------> The type of model architecture used is Skip-Grams that predict the context input from the target input\n",
    "\n",
    "   (f.)    epochs = 100 ---------------> 100 iterations is needed to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f201b",
   "metadata": {},
   "source": [
    "### Step 7:  Explore Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "462cb569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00735378,  0.00497393,  0.01114157,  0.01594871, -0.01240279,\n",
       "        0.01517002,  0.01622976,  0.02488607,  0.007512  , -0.01110035,\n",
       "       -0.00865914, -0.00417537,  0.00180949,  0.02212013,  0.00360116,\n",
       "        0.0115539 ,  0.01197331, -0.01301393,  0.0101827 ,  0.01140497,\n",
       "       -0.00013241,  0.00652788, -0.00716085, -0.01081918,  0.01669241,\n",
       "       -0.00234791,  0.01293053,  0.00885004, -0.00451266,  0.00574325,\n",
       "       -0.01230733, -0.00705541,  0.00379279,  0.00600469, -0.00319157,\n",
       "        0.00208915, -0.01008951,  0.01859779, -0.0193346 ,  0.0138065 ,\n",
       "        0.01430457, -0.01492667, -0.0115025 , -0.01158754, -0.0054677 ,\n",
       "        0.01225077,  0.01911482,  0.00240991,  0.00264861,  0.02230951],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the numerical dense dimensional representation of the 'Natural' of size 50\n",
    "\n",
    "skipgrams_model.wv['natural']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8b9da",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. We have obtained the numerical dense dimensional vector for the word 'natural' of the size 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bb902e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.76391884e-04,  9.13006905e-03,  4.92043234e-03,  2.16485839e-02,\n",
       "        3.67378561e-05, -1.14126895e-02,  3.65891145e-03,  1.16187232e-02,\n",
       "       -9.19059757e-03, -2.18484420e-02, -1.32288588e-02, -1.69963464e-02,\n",
       "        1.99381914e-02,  1.11467065e-02,  1.79607235e-02,  1.81943681e-02,\n",
       "       -5.48013020e-03,  1.20153315e-02,  1.25141107e-02, -3.99846351e-03,\n",
       "        1.08411601e-02,  1.44450553e-02, -9.62403044e-03,  2.03738566e-02,\n",
       "        1.39736291e-02,  2.21483465e-02, -1.73616484e-02,  2.04764623e-02,\n",
       "        1.22453421e-02, -2.82903155e-03, -1.87412500e-02,  6.22507511e-03,\n",
       "       -1.21589610e-02, -1.61428545e-02,  6.14509825e-03,  1.72968823e-02,\n",
       "        2.51329802e-02, -9.67694167e-03, -2.09322739e-02,  1.90725122e-02,\n",
       "        2.16437131e-02, -9.07084811e-03, -1.84230395e-02, -3.08223884e-03,\n",
       "       -8.95738485e-04, -9.18841735e-03,  9.08656744e-04, -3.82377347e-03,\n",
       "       -4.78508603e-03, -7.82969035e-03], dtype=float32)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the numerical dense dimensional representation of the 'and' of size 50\n",
    "\n",
    "skipgrams_model.wv['and']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc080c7",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. We have obtained the numerical dense dimensional vector for the word 'and' of the size 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c071f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01449395,  0.01274676, -0.01908677,  0.00811913, -0.01309085,\n",
       "       -0.01845687,  0.00691228,  0.00757935, -0.01610652, -0.01883319,\n",
       "       -0.00672934,  0.01057118, -0.00780058,  0.01911717,  0.00090596,\n",
       "        0.00747844,  0.02598679,  0.0165633 , -0.01177003, -0.0002098 ,\n",
       "        0.01229269,  0.01556609,  0.00862363, -0.00683072,  0.00299018,\n",
       "       -0.00575807,  0.0019207 ,  0.01826918,  0.00919217,  0.0179157 ,\n",
       "       -0.0222159 , -0.01529316,  0.01461737, -0.00303525, -0.01417821,\n",
       "        0.00309936,  0.01374001, -0.0056397 , -0.01244262,  0.00963698,\n",
       "        0.02909352, -0.01626427, -0.00224196,  0.00165029, -0.00580131,\n",
       "       -0.00549196, -0.01131593,  0.01492768, -0.0099291 ,  0.01232163],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the numerical dense dimensional representation of the 'machine' of size 50\n",
    "\n",
    "skipgrams_model.wv['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21350d0",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. We have obtained the numerical dense dimensional vector for the word 'machine' of the size 50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd473be",
   "metadata": {},
   "source": [
    "### Step 8: Find out the vector representation of the words that are most similar to the words 'and' and 'machine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5b1265cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 0.4520156681537628),\n",
       " ('used', 0.45019078254699707),\n",
       " ('by', 0.43641555309295654),\n",
       " ('a', 0.4132101535797119),\n",
       " ('related', 0.4129936397075653),\n",
       " ('be', 0.40556907653808594),\n",
       " ('to', 0.38448694348335266),\n",
       " ('word2vec', 0.3798808753490448),\n",
       " ('machine', 0.37022748589515686),\n",
       " ('learning', 0.36773473024368286)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dense dimensional vectors for the words that are most similar to 'and'\n",
    "\n",
    "skipgrams_model.wv.most_similar('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74053abc",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. All the above words are the most similar w.r.to the word 'and' as their vector values are similar w.r.to 'and'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0134cfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.5157498717308044),\n",
       " ('a', 0.46377620100975037),\n",
       " ('are', 0.43209725618362427),\n",
       " ('word2vec', 0.42735937237739563),\n",
       " ('creating', 0.3958236575126648),\n",
       " ('high-quality', 0.3800097107887268),\n",
       " ('and', 0.37022748589515686),\n",
       " ('word', 0.3700869083404541),\n",
       " ('processing', 0.3548687994480133),\n",
       " ('training', 0.3465283215045929)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dense dimensional vectors for the words that are most similar to 'machine'\n",
    "\n",
    "skipgrams_model.wv.most_similar('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fd589",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. All the above words are the most similar w.r.to the word 'machine' as their vector values are similar w.r.to 'machine'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac7c09",
   "metadata": {},
   "source": [
    "### Step 9: To show the list of all the words that the model has learnt for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4eace3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'word': 1,\n",
       " 'are': 2,\n",
       " 'training': 3,\n",
       " 'for': 4,\n",
       " 'models': 5,\n",
       " 'be': 6,\n",
       " 'words': 7,\n",
       " 'of': 8,\n",
       " 'a': 9,\n",
       " 'embeddings': 10,\n",
       " 'learning': 11,\n",
       " 'python': 12,\n",
       " 'in': 13,\n",
       " 'word2vec': 14,\n",
       " 'library': 15,\n",
       " 'popular': 16,\n",
       " 'is': 17,\n",
       " 'gensim': 18,\n",
       " 'datasets': 19,\n",
       " 'text': 20,\n",
       " 'large': 21,\n",
       " 'on': 22,\n",
       " 'by': 23,\n",
       " 'captured': 24,\n",
       " 'can': 25,\n",
       " 'meaning': 26,\n",
       " 'the': 27,\n",
       " 'high-quality': 28,\n",
       " 'creating': 29,\n",
       " 'used': 30,\n",
       " 'deep': 31,\n",
       " 'vectors': 32,\n",
       " 'as': 33,\n",
       " 'represented': 34,\n",
       " 'to': 35,\n",
       " 'allows': 36,\n",
       " 'that': 37,\n",
       " 'representation': 38,\n",
       " 'type': 39,\n",
       " 'fields': 40,\n",
       " 'related': 41,\n",
       " 'closely': 42,\n",
       " 'machine': 43,\n",
       " 'and': 44,\n",
       " 'processing': 45,\n",
       " 'language': 46,\n",
       " 'natural': 47}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd3271",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. key_to_index ---> It helps in mapping every word present in the vocabulary to its unique integer index that is trained using the Word2Vec Model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
