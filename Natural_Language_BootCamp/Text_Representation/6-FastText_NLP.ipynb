{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84944906",
   "metadata": {},
   "source": [
    "### FastText\n",
    "\n",
    "1.  Fast Text is an improvised version of Word2Vec Model.\n",
    "\n",
    "2.  It considers every word as atomic and divides the words into the subwords.\n",
    "\n",
    "3.  It handles every misspelled words and also handles the morphological variations in the words.\n",
    "\n",
    "\n",
    "### Importance of Fasttext:\n",
    "\n",
    "1.  It breaks the words into the sub-words.\n",
    "\n",
    "2.  It handles all the mis-spelled words in the text.\n",
    "\n",
    "3.  It handles all the morphological variations in the words.\n",
    "\n",
    "4.  The main task of FastText is that it can deduce the vector form of some unknown words which is not there in the vocabulary list based on some information of some sub-words present in the vocabulary list.\n",
    "\n",
    "### Steps used in this Algorithm:---\n",
    "\n",
    "1.  Import all the necessary libraries\n",
    "\n",
    "2.  Download the necessary NLTK resources\n",
    "\n",
    "3.  Prepare Your Text Data\n",
    "\n",
    "4.  Perform the Tokenization on the corpus text\n",
    "\n",
    "5.  Perform the normalization on the text\n",
    "\n",
    "6.  Remove the stopwords from the text\n",
    "\n",
    "7.  Train the FastText Model\n",
    "\n",
    "8.  Explore the Trained Model\n",
    "\n",
    "9.  Handle Out-of-Vocabulary (OOV) Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90664b",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "77556f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nltk\n",
    "\n",
    "from    nltk.tokenize   import  word_tokenize\n",
    "\n",
    "import  numpy  as  np\n",
    "\n",
    "from    gensim.models  import  FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126a4dd",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  nltk ------------->  Library for Text Preprocessing\n",
    "\n",
    "2.  tokenize ---------> tokenization purpose(breaking the parts into sub-parts)\n",
    "\n",
    "3.  word_tokenize ----> breaks the word into sub-words\n",
    "\n",
    "4.  numpy ------------> Computation of numerical array\n",
    "\n",
    "5.  gensim -----------> It has the module for the implementation of FastText\n",
    "\n",
    "6.  FastText ---------> helps in finding out the vector form of unknown words based on the known words and can handle mis-spelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8f38a",
   "metadata": {},
   "source": [
    "### Step 2: Download the necessary NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "43e17817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger_eng: Package\n",
      "[nltk_data]     'average_perceptron_tagger_eng' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('average_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f26a06",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  punkt_tab is a tokenization module\n",
    "\n",
    "2.  average_perceptron_tagger_eng  is a POS Tagging Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889563be",
   "metadata": {},
   "source": [
    "### Step 3:  Prepare Your Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "79273f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural language processing is a fascinating field\",\n",
    "    \"Machine learning enables computers to learn from data\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Word embeddings represent words as vectors\",\n",
    "    \"FastText handles rare and unseen words effectively\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "864a8977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing is a fascinating field',\n",
       " 'Machine learning enables computers to learn from data',\n",
       " 'Deep learning is a subset of machine learning',\n",
       " 'Word embeddings represent words as vectors',\n",
       " 'FastText handles rare and unseen words effectively']"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "61ccf40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e1fb01af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing is a fascinating field Machine learning enables computers to learn from data Deep learning is a subset of machine learning Word embeddings represent words as vectors FastText handles rare and unseen words effectively'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2479b",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. A corpus is a container that contains the set of two or more texts in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef447b",
   "metadata": {},
   "source": [
    "### Step 4: Perform the Tokenization on the corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "819fad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "c7b200eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " 'Machine',\n",
       " 'learning',\n",
       " 'enables',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'Word',\n",
       " 'embeddings',\n",
       " 'represent',\n",
       " 'words',\n",
       " 'as',\n",
       " 'vectors',\n",
       " 'FastText',\n",
       " 'handles',\n",
       " 'rare',\n",
       " 'and',\n",
       " 'unseen',\n",
       " 'words',\n",
       " 'effectively']"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6265520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing is a fascinating field Machine learning enables computers to learn from data Deep learning is a subset of machine learning Word embeddings represent words as vectors FastText handles rare and unseen words effectively\n"
     ]
    }
   ],
   "source": [
    "words = \" \".join(words)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c2e83",
   "metadata": {},
   "source": [
    "### Step 5: Perform the normalization on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "03c80057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'Machine', 'learning', 'enables', 'computers', 'to', 'learn', 'from', 'data', 'Deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'Word', 'embeddings', 'represent', 'words', 'as', 'vectors', 'FastText', 'handles', 'rare', 'and', 'unseen', 'words', 'effectively']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "### Create the object for Regular Expression Tokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "res = reg.tokenize(words)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c4b92",
   "metadata": {},
   "source": [
    "### Step 6: Remove the stopwords from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "c5421aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------List of all the english stopwords----------------------------------------------------\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "-------------------------------List of all the Filtered words----------------------------------------------------\n",
      "['Natural', 'language', 'processing', 'fascinating', 'field', 'Machine', 'learning', 'enables', 'computers', 'learn', 'data', 'Deep', 'learning', 'subset', 'machine', 'learning', 'Word', 'embeddings', 'represent', 'words', 'vectors', 'FastText', 'handles', 'rare', 'unseen', 'words', 'effectively']\n"
     ]
    }
   ],
   "source": [
    "### define the enlish stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "print(\"-------------------------------List of all the english stopwords----------------------------------------------------\")\n",
    "\n",
    "print(english_stopwords)\n",
    "\n",
    "\n",
    "### filter and remove all the stop words from the text\n",
    "\n",
    "res = [x for x in res if(x not in english_stopwords)]\n",
    "\n",
    "print(\"-------------------------------List of all the Filtered words----------------------------------------------------\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "1ac34977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field'], ['machine', 'learning', 'enables', 'computers', 'to', 'learn', 'from', 'data'], ['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning'], ['word', 'embeddings', 'represent', 'words', 'as', 'vectors'], ['fasttext', 'handles', 'rare', 'and', 'unseen', 'words', 'effectively']]\n"
     ]
    }
   ],
   "source": [
    "### Tokenized sentences\n",
    "\n",
    "corpus = [\n",
    "    \"Natural language processing is a fascinating field\",\n",
    "    \"Machine learning enables computers to learn from data\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Word embeddings represent words as vectors\",\n",
    "    \"FastText handles rare and unseen words effectively\"\n",
    "]\n",
    "\n",
    "tokenized_sentences = [word_tokenize(x.lower()) for x in corpus]\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758b70a",
   "metadata": {},
   "source": [
    "### Step 7: Train the FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b3cadc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText(\n",
    "    sentences    =      tokenized_sentences                  ,\n",
    "    vector_size  =          50                               ,\n",
    "    window       =           3                               ,\n",
    "    min_count    =           1                               ,\n",
    "    sg           =           1                               ,\n",
    "    epochs       =          100              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "faadbc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x10c558b4e20>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b1c3f",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  The FastText model is trained with the help of the following input parameters:---\n",
    "\n",
    "     (a.)   sentences = tokenized_sentences -----------> Input is in the form of the list of sentences\n",
    "\n",
    "     (b.)   vector_size = 50    -----------------------> Each sentence dense dimenstional vector size should be 50\n",
    "\n",
    "     (c.)   window      = 3     ------------------------> the number of inputs for the context mapping input should be 3\n",
    "\n",
    "     (d.)   min_count   = 1     ------------------------>  minimum number of inputs should be 1\n",
    "\n",
    "     (e.)   sg   = 1            ------------------------> The model architecture for the FastText model should be  Skip-Grams\n",
    "\n",
    "     (f.)   epochs = 100         -----------------------> No of iterations needed to run the model is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4483a",
   "metadata": {},
   "source": [
    "### Step 8: Explore the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5cab778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.2463701e-04, -5.3047930e-04,  6.6479814e-04, -1.2593132e-03,\n",
       "        3.0367132e-03, -2.3750232e-03,  4.7879447e-03,  2.8217584e-03,\n",
       "       -1.8424509e-04,  2.9925893e-03, -3.1626706e-03, -1.3293079e-03,\n",
       "        1.6390678e-03,  1.4841135e-03,  2.2499785e-03, -8.2287972e-04,\n",
       "       -2.3260086e-03,  6.3928356e-04, -2.3356080e-03, -4.8548065e-04,\n",
       "       -8.5085107e-04,  2.4301901e-03, -1.5599162e-03, -1.4388099e-03,\n",
       "        3.9753891e-03, -1.0512471e-03, -3.0597935e-03, -2.0209956e-03,\n",
       "       -8.8080805e-04, -3.6776997e-04,  2.1390072e-03,  2.8840434e-03,\n",
       "       -1.3368093e-05, -1.0970196e-03, -2.0248929e-04, -2.4140853e-04,\n",
       "        4.1443360e-04, -4.2922402e-04, -1.4309796e-03, -9.1862434e-04,\n",
       "        1.2138172e-03,  2.1118002e-03, -4.0835668e-03,  1.6549744e-03,\n",
       "       -4.9756712e-04, -3.3199308e-03, -2.2702778e-03, -9.2173531e-04,\n",
       "        7.1208744e-04,  1.6569267e-03], dtype=float32)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Vector representation of the word 'Natural'\n",
    "\n",
    "fasttext_model.wv['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "c6e49eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8151738e-03,  1.8731835e-03, -1.3348084e-03,  1.3475234e-03,\n",
       "        1.3195594e-03,  2.3620769e-03, -1.6945349e-03,  4.7780303e-03,\n",
       "       -2.8437979e-03,  1.1530526e-03, -3.7804700e-03,  1.4331297e-04,\n",
       "        2.8665448e-03,  9.4495638e-04,  1.8654962e-03,  2.3849651e-03,\n",
       "       -3.0143530e-04,  2.7725664e-03,  4.1818535e-03,  1.6576212e-04,\n",
       "       -3.2666209e-03, -3.6462671e-03,  4.1487538e-03, -2.0292855e-03,\n",
       "       -6.5047789e-04,  1.5003482e-03, -1.6418330e-03, -2.2933257e-03,\n",
       "        1.2275674e-04,  2.1315969e-03, -2.4044698e-03, -2.2868451e-03,\n",
       "        2.7977030e-03, -1.7129083e-03,  5.4581878e-03,  4.6377955e-03,\n",
       "        3.1717520e-03,  2.6782481e-03, -2.6908063e-03,  1.2677161e-03,\n",
       "       -2.5304388e-03,  3.3428398e-04,  1.1895931e-03, -5.2325479e-03,\n",
       "        2.4010935e-03, -5.8245561e-03,  5.0611510e-05, -2.2862332e-03,\n",
       "        1.8495488e-03, -4.6029566e-03], dtype=float32)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Vector representation of the word 'Machine'\n",
    "\n",
    "fasttext_model.wv['machine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "05bf86ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.2241930e-03,  3.8676456e-04, -3.7578144e-03, -1.0782945e-03,\n",
       "        8.5799803e-04,  4.8130631e-04, -2.7068353e-03,  1.7609344e-03,\n",
       "        3.4147568e-04,  5.3201732e-03,  2.5767388e-03, -8.7948246e-03,\n",
       "       -1.9125718e-03,  1.4498566e-03, -3.6431815e-05,  5.0643785e-03,\n",
       "       -5.6656729e-03, -1.2239072e-03,  1.7446767e-03,  2.0881162e-03,\n",
       "        3.7119447e-04,  9.0195914e-04, -1.7362975e-03, -1.8576275e-03,\n",
       "        9.7739801e-04,  8.3807507e-04, -4.0719640e-03,  1.5394125e-03,\n",
       "        3.6462902e-03, -9.2380783e-03,  4.0885871e-03, -2.8355708e-03,\n",
       "       -3.6883189e-03,  5.9062638e-04, -9.2948861e-03,  3.8188382e-03,\n",
       "       -3.9598304e-03, -3.2933389e-03, -2.1968626e-03,  1.1714228e-03,\n",
       "       -5.3502065e-03,  3.9881472e-03,  3.6802196e-03, -6.3529387e-03,\n",
       "       -2.1181349e-04, -1.0554730e-03, -4.1337675e-04,  4.3161181e-03,\n",
       "       -2.1706399e-05,  8.1578776e-04], dtype=float32)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Vector representation of the word 'Deep'\n",
    "\n",
    "fasttext_model.wv['deep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "a90f79db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learn', 0.566283643245697),\n",
       " ('embeddings', 0.3166630268096924),\n",
       " ('processing', 0.3066617250442505),\n",
       " ('is', 0.2814252972602844),\n",
       " ('language', 0.22058233618736267),\n",
       " ('fascinating', 0.1873551607131958),\n",
       " ('computers', 0.1703764796257019),\n",
       " ('natural', 0.1666349172592163),\n",
       " ('and', 0.1589219719171524),\n",
       " ('as', 0.14239010214805603)]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find most similar words to a given word 'learning'\n",
    "\n",
    "fasttext_model.wv.most_similar('learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d52b17",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The above are the list of words that are most similar w.r.to the word 'learning' as they have high vector values and is nearer to the word 'learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "8f886b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rare', 0.4053672254085541),\n",
       " ('of', 0.26202529668807983),\n",
       " ('handles', 0.14107008278369904),\n",
       " ('and', 0.07821687310934067),\n",
       " ('word', 0.07267072051763535),\n",
       " ('enables', 0.06929602473974228),\n",
       " ('language', 0.05143461003899574),\n",
       " ('processing', 0.031261101365089417),\n",
       " ('represent', 0.028187723830342293),\n",
       " ('unseen', 0.027802225202322006)]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find most similar words to a given word 'machine'\n",
    "\n",
    "fasttext_model.wv.most_similar('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52d3a3",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The above are the list of words that are most similar w.r.to the word 'learning' as they have high vector values and is nearer to the word 'machine'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32932a6d",
   "metadata": {},
   "source": [
    "### Step 9: Handle Out-of-Vocabulary (OOV) Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4cb92",
   "metadata": {},
   "source": [
    "This is where FastText comes into the picture where it can deduce the vector form of some unknown words not present in the vocabulary list using the information of some subwords present in the vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e422149d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.6743815e-04, -9.2491199e-04, -1.3378768e-03, -1.8773484e-03,\n",
       "       -5.4910220e-04,  1.5938377e-03, -7.2624895e-04, -1.0992755e-03,\n",
       "       -1.9265991e-03,  5.2845478e-04, -2.2062492e-04, -3.9157597e-03,\n",
       "        3.1014791e-04,  2.5739563e-03, -1.4379311e-03, -1.1849512e-03,\n",
       "        6.8146852e-05, -1.3054026e-03, -1.9864601e-03,  3.4175890e-03,\n",
       "        4.0605391e-04, -4.9657107e-04, -7.8516663e-04,  1.0991159e-03,\n",
       "       -1.3701107e-03, -3.7043048e-03, -2.8391177e-04,  3.4112758e-03,\n",
       "       -2.7216771e-03,  5.7162889e-03,  1.1733305e-03,  1.8359000e-03,\n",
       "       -2.6961204e-03,  2.9323995e-03, -7.0885126e-04, -2.5519500e-03,\n",
       "        1.7971118e-03, -9.7494310e-04,  2.7351868e-03,  4.3866527e-03,\n",
       "        2.2974834e-03,  6.8506144e-04,  2.4359836e-03, -7.4982658e-05,\n",
       "        1.2949929e-03,  1.5505051e-03, -2.9311841e-03, -2.3272594e-03,\n",
       "       -2.3558319e-03,  2.7747609e-04], dtype=float32)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find the vector representation of the word 'learnings' that is unknown\n",
    "\n",
    "fasttext_model.wv['learnings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3806e",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. learnings is the word not present in the vocabulary list.\n",
    "\n",
    "2. As here Fastext model is trained, so with the help of the sub word (learning) present in the vocabulary,\n",
    "learnings which is not present in the vocabulary list also gets trained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0577cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar words to 'data' from loaded model:\n",
      "[('represent', 0.40004679560661316), ('to', 0.28480449318885803), ('is', 0.26224812865257263), ('from', 0.24866099655628204), ('and', 0.15458159148693085), ('enables', 0.15229645371437073), ('handles', 0.14785687625408173), ('fascinating', 0.09904083609580994), ('effectively', 0.09806496649980545), ('subset', 0.09262485802173615)]\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "fasttext_model.save(\"fasttext_model.bin\")\n",
    "\n",
    "# Load model later\n",
    "loaded_model = FastText.load(\"fasttext_model.bin\")\n",
    "\n",
    "# Verify loaded model\n",
    "print(\"\\nSimilar words to 'data' from loaded model:\")\n",
    "print(loaded_model.wv.most_similar('data'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
