{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12ca684",
   "metadata": {},
   "source": [
    "### Word-Embeddings:\n",
    "\n",
    "Word-Embeddings are the  numerical dense valued representations of the words in the text. It helps in finding out the similarity between two words in the text and the similar words have the same dense vector values.\n",
    "\n",
    "It helps in finding out the relation between the words in the text.\n",
    "\n",
    "It captures the similarity between the two words in the text in the form of vectors.\n",
    "\n",
    "\n",
    "### Importances of Word-Embeddings:---\n",
    "\n",
    "1. It is used to find out the similarity score between two word in the text in the form of dense dimensional vectors.\n",
    "\n",
    "2. It is used to find out the meaning behind every text.\n",
    "\n",
    "3. It is useful for dimensionality reduction.\n",
    "\n",
    "4. It is used to improve the accuracy and the performance of the model\n",
    "\n",
    "5. It helps in finding out the relation between the words in th etext and even finds out which words are more similar. It is also represented in the form of similarity score dense dimensional vectors.\n",
    "\n",
    "\n",
    "\n",
    "Ex:---\n",
    "\n",
    "1.   king - man + woman = queen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca1362",
   "metadata": {},
   "source": [
    "### Write a simple end to end implementation based on Word Embeddings\n",
    "\n",
    "\n",
    "#### Steps used in this Algorithm :----\n",
    "\n",
    "1.   Import all the necessary libraries\n",
    "\n",
    "2.   Download all the necessary libraries for NLTK\n",
    "\n",
    "3.   Define the corpus text\n",
    "\n",
    "4.   Perform the normalization of the text\n",
    "\n",
    "5.   Perform the Word tokenization on the text\n",
    "\n",
    "6.   Train the Word2Vec Model\n",
    "\n",
    "7.   Explore Word Vectors\n",
    "\n",
    "8.   Find out the vector representation of the words that are most similar to the words 'and' and 'machine'\n",
    "\n",
    "9.   To show the list of all the words that the model has learnt for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce98f4",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "726b7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy   as   np\n",
    "\n",
    "from    nltk.tokenize   import  word_tokenize\n",
    "\n",
    "from    gensim.models   import Word2Vec\n",
    "\n",
    "import  nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088ca59",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  numpy ------------->  Computation of numerical array\n",
    "\n",
    "2.  word_tokenize ------>  breaks the sentences into words\n",
    "\n",
    "3.  gensim ------------>  a module used for embedding\n",
    "\n",
    "4.  Word2Vec ---------->  finds out the relation between the word and mentions the relation in form of numerical dense dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dbd9c9",
   "metadata": {},
   "source": [
    "### Step 2: Download all the necessary libraries for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "1f36f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger_eng: Package\n",
      "[nltk_data]     'average_perceptron_tagger_eng' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk.download('average_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2ac6e",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Here we are downloading the additional resources needed for NLTK libraries\n",
    "\n",
    "    (a.)  punkt_tab ----------->  Tokenization model\n",
    "\n",
    "    (b.)  average_perceptron_tagger_eng  ---------->  POS Tagging Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb472d81",
   "metadata": {},
   "source": [
    "### Step 3:  Define the corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "7422c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing and Machine Learning are closely related fields.\",\n",
    "    \"Word embeddings are a type of word representation that allows words to be represented as vectors.\",\n",
    "    \"Deep learning models are used for creating high-quality word embeddings.\",\n",
    "    \"The meaning of words can be captured by training on large text datasets.\",\n",
    "    \"Gensim is a popular library for training Word2Vec models in Python.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "06a0de39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing and Machine Learning are closely related fields.',\n",
       " 'Word embeddings are a type of word representation that allows words to be represented as vectors.',\n",
       " 'Deep learning models are used for creating high-quality word embeddings.',\n",
       " 'The meaning of words can be captured by training on large text datasets.',\n",
       " 'Gensim is a popular library for training Word2Vec models in Python.']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc50dc",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1.  A corpus ia a container that has a series of sentences needed for text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f200a6",
   "metadata": {},
   "source": [
    "### Step 4: Perform the normalization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1d77a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert the courpus in list to string\n",
    "\n",
    "corpus = \" \".join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "93af639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'and', 'Machine', 'Learning', 'are', 'closely', 'related', 'fields', 'Word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'to', 'be', 'represented', 'as', 'vectors', 'Deep', 'learning', 'models', 'are', 'used', 'for', 'creating', 'high', 'quality', 'word', 'embeddings', 'The', 'meaning', 'of', 'words', 'can', 'be', 'captured', 'by', 'training', 'on', 'large', 'text', 'datasets', 'Gensim', 'is', 'a', 'popular', 'library', 'for', 'training', 'Word2Vec', 'models', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "from  nltk.tokenize  import RegexpTokenizer\n",
    "\n",
    "### Create the object for Regular Expression Tokenizer\n",
    "\n",
    "reg = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "### using the object for Regular Expression Tokenizer, normalize the text\n",
    "\n",
    "ans = reg.tokenize(corpus)\n",
    "\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638b719",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. Now using Regular Expression Tokenizer, all the punctuations and special symbols have been removed from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c06b2f",
   "metadata": {},
   "source": [
    "### Step 5: Perform the Word tokenization on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "79a4ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Tokenization breaks the sentences into words. So the input is in the form of text\n",
    "\n",
    "ans = \" \".join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "c3169407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing and Machine Learning are closely related fields Word embeddings are a type of word representation that allows words to be represented as vectors Deep learning models are used for creating high quality word embeddings The meaning of words can be captured by training on large text datasets Gensim is a popular library for training Word2Vec models in Python'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "510f197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize  import word_tokenize\n",
    "\n",
    "\n",
    "words = word_tokenize(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "333635f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'and', 'Machine', 'Learning', 'are', 'closely', 'related', 'fields', 'Word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'to', 'be', 'represented', 'as', 'vectors', 'Deep', 'learning', 'models', 'are', 'used', 'for', 'creating', 'high', 'quality', 'word', 'embeddings', 'The', 'meaning', 'of', 'words', 'can', 'be', 'captured', 'by', 'training', 'on', 'large', 'text', 'datasets', 'Gensim', 'is', 'a', 'popular', 'library', 'for', 'training', 'Word2Vec', 'models', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "8be13313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural', 'language', 'processing', 'and', 'machine', 'learning', 'are', 'closely', 'related', 'fields', '.'], ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'to', 'be', 'represented', 'as', 'vectors', '.'], ['deep', 'learning', 'models', 'are', 'used', 'for', 'creating', 'high-quality', 'word', 'embeddings', '.'], ['the', 'meaning', 'of', 'words', 'can', 'be', 'captured', 'by', 'training', 'on', 'large', 'text', 'datasets', '.'], ['gensim', 'is', 'a', 'popular', 'library', 'for', 'training', 'word2vec', 'models', 'in', 'python', '.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing and Machine Learning are closely related fields.\",\n",
    "    \"Word embeddings are a type of word representation that allows words to be represented as vectors.\",\n",
    "    \"Deep learning models are used for creating high-quality word embeddings.\",\n",
    "    \"The meaning of words can be captured by training on large text datasets.\",\n",
    "    \"Gensim is a popular library for training Word2Vec models in Python.\"\n",
    "]\n",
    "\n",
    "\n",
    "### For the Word2Vec model, the input must be in the form of list of sentences\n",
    "\n",
    "tokenized_sentences = [ word_tokenize(x.lower()) for x in corpus]\n",
    "\n",
    "print(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bef59f",
   "metadata": {},
   "source": [
    "### Step 6: Train the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3a158928",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(\n",
    "    sentences           =           tokenized_sentences           ,         ### all inputs in form of list of sentences\n",
    "    vector_size         =               50                        ,         ### every word tokens is in dense dimensional vectors\n",
    "    window              =               3                         ,         ### size of the context word needed for embedding\n",
    "    min_count           =               1                         ,         ### keeps all the words onnce with frequency  < 1\n",
    "    sg                  =               0                         ,         ### sg defines the tyupe of architecture\n",
    "    epochs              =             100                                   ### no of iteraations needed to train the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "bc0dfa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x188cabe5150>"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd4028",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The object for cbow model is created\n",
    "\n",
    "2. The model is trained with the help of following parameters:---\n",
    "\n",
    "    (a.)  sentences -------->  tokenized words (It accepts the tokenized words as the input)\n",
    "\n",
    "    (b.)  vector_size ------>  Every tokenized input is in form of 50 size dense dimensional vectors\n",
    "\n",
    "    (c.)  window     -------> size of the context input embeddings\n",
    "\n",
    "    (d.)  min_count ---------> keeps all the input context embeddings as intact\n",
    "\n",
    "    (e.)  sg ----------------> If sg = 0 then Continuous Bag Of Words\n",
    "\n",
    "                               takes every context input\n",
    "                                \n",
    "                               converts every input into dense embedding vectors\n",
    "\n",
    "                               considers every input vector in 1d row vector\n",
    "\n",
    "                               take the average to find out the target word using the context wod\n",
    "\n",
    "    (f.) epochs = 100 -----------> 100 iteartions needed to tarin the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28db6d",
   "metadata": {},
   "source": [
    "### Step 7:  Explore Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "76c125b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00699806,  0.00469796,  0.01108254,  0.01569879, -0.01242217,\n",
       "        0.01562483,  0.01341129,  0.02114157,  0.00826154, -0.00840245,\n",
       "       -0.00803872, -0.0024587 , -0.00141286,  0.02039655,  0.00273099,\n",
       "        0.00932626,  0.00830445, -0.01621871,  0.01216197,  0.01455295,\n",
       "       -0.00260283,  0.00551177, -0.01120573, -0.01162836,  0.01644766,\n",
       "       -0.00464357,  0.01297208,  0.00513226, -0.00401876,  0.00629348,\n",
       "       -0.00951927, -0.00496117,  0.00329364,  0.00802115, -0.0031375 ,\n",
       "        0.00164507, -0.01367713,  0.01896198, -0.01835306,  0.01440091,\n",
       "        0.01012293, -0.0143893 , -0.01136863, -0.0143218 , -0.00856171,\n",
       "        0.01328081,  0.01916406,  0.00350576,  0.00128381,  0.01994897],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the numerical dense dimensional representation of the 'Natural' of size 50\n",
    "\n",
    "cbow_model.wv['natural']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a595479",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The word 'natural' is represented in the form of numerical dense dimensisonal vector of size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "763a1f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00018774,  0.0090975 ,  0.00483873,  0.02077695,  0.00037773,\n",
       "       -0.01141587, -0.00036813,  0.00639544, -0.00805818, -0.01825272,\n",
       "       -0.01182912, -0.01532571,  0.01545134,  0.00883123,  0.01715365,\n",
       "        0.0151855 , -0.00994112,  0.0078078 ,  0.01504973,  0.00072297,\n",
       "        0.00741111,  0.0129896 , -0.01509411,  0.0189632 ,  0.013627  ,\n",
       "        0.01904808, -0.01716955,  0.01510367,  0.01333817, -0.00276821,\n",
       "       -0.01487494,  0.0085975 , -0.01309427, -0.01296896,  0.00653372,\n",
       "        0.01653673,  0.02061739, -0.00869386, -0.01938032,  0.01941955,\n",
       "        0.01604813, -0.00865967, -0.01779248, -0.00709295, -0.00549124,\n",
       "       -0.00767929,  0.00133798, -0.00175478, -0.00598082, -0.0106355 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv['and']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1e166",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The word 'and' is represented in the form of numerical dense dimensional vector of the size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "9259efa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.38767501e-02,  1.27547709e-02, -1.94403455e-02,  7.10700080e-03,\n",
       "       -1.30583849e-02, -1.88706126e-02,  2.37601483e-03,  1.51365413e-03,\n",
       "       -1.54858893e-02, -1.52489319e-02, -5.61180850e-03,  1.24633871e-02,\n",
       "       -1.23069454e-02,  1.63674857e-02,  2.08636593e-05,  4.30864701e-03,\n",
       "        2.13472918e-02,  1.16836606e-02, -9.04008280e-03,  4.99267038e-03,\n",
       "        8.19391292e-03,  1.39079476e-02,  2.84056971e-03, -8.08171555e-03,\n",
       "        2.72642379e-03, -8.81033670e-03,  1.97464367e-03,  1.27054192e-02,\n",
       "        1.05124600e-02,  1.86052434e-02, -1.72608197e-02, -1.30702471e-02,\n",
       "        1.37978848e-02,  1.97771340e-04, -1.37183741e-02,  2.25852989e-03,\n",
       "        8.79017171e-03, -5.04150009e-03, -1.11420834e-02,  1.01354253e-02,\n",
       "        2.26622969e-02, -1.54554360e-02, -1.14744401e-03, -2.99247587e-03,\n",
       "       -1.05237961e-02, -4.00301535e-03, -1.06647415e-02,  1.71461608e-02,\n",
       "       -1.16504729e-02,  8.87040980e-03], dtype=float32)"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718458e",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. The word 'machine' is represented in the form of numerical dense dimensional vector of the size 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30a9db",
   "metadata": {},
   "source": [
    "### Step 8: Find out the vector representation of the words that are most similar to the words 'and' and 'machine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "27d95ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00018774,  0.0090975 ,  0.00483873,  0.02077695,  0.00037773,\n",
       "       -0.01141587, -0.00036813,  0.00639544, -0.00805818, -0.01825272,\n",
       "       -0.01182912, -0.01532571,  0.01545134,  0.00883123,  0.01715365,\n",
       "        0.0151855 , -0.00994112,  0.0078078 ,  0.01504973,  0.00072297,\n",
       "        0.00741111,  0.0129896 , -0.01509411,  0.0189632 ,  0.013627  ,\n",
       "        0.01904808, -0.01716955,  0.01510367,  0.01333817, -0.00276821,\n",
       "       -0.01487494,  0.0085975 , -0.01309427, -0.01296896,  0.00653372,\n",
       "        0.01653673,  0.02061739, -0.00869386, -0.01938032,  0.01941955,\n",
       "        0.01604813, -0.00865967, -0.01779248, -0.00709295, -0.00549124,\n",
       "       -0.00767929,  0.00133798, -0.00175478, -0.00598082, -0.0106355 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "94f6afec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('used', 0.3690727949142456),\n",
       " ('by', 0.31513360142707825),\n",
       " ('are', 0.2984626293182373),\n",
       " ('a', 0.2782883644104004),\n",
       " ('related', 0.2708548903465271),\n",
       " ('to', 0.26222237944602966),\n",
       " ('be', 0.23448440432548523),\n",
       " ('processing', 0.23242348432540894),\n",
       " ('machine', 0.23120737075805664),\n",
       " ('language', 0.2278352528810501)]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Most similar to 'and'\n",
    "\n",
    "cbow_model.wv.most_similar('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d1333",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. All the above words are the most similar w.r.to the word 'and' as their vector values are similar w.r.to 'and'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "429a276d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.38767501e-02,  1.27547709e-02, -1.94403455e-02,  7.10700080e-03,\n",
       "       -1.30583849e-02, -1.88706126e-02,  2.37601483e-03,  1.51365413e-03,\n",
       "       -1.54858893e-02, -1.52489319e-02, -5.61180850e-03,  1.24633871e-02,\n",
       "       -1.23069454e-02,  1.63674857e-02,  2.08636593e-05,  4.30864701e-03,\n",
       "        2.13472918e-02,  1.16836606e-02, -9.04008280e-03,  4.99267038e-03,\n",
       "        8.19391292e-03,  1.39079476e-02,  2.84056971e-03, -8.08171555e-03,\n",
       "        2.72642379e-03, -8.81033670e-03,  1.97464367e-03,  1.27054192e-02,\n",
       "        1.05124600e-02,  1.86052434e-02, -1.72608197e-02, -1.30702471e-02,\n",
       "        1.37978848e-02,  1.97771340e-04, -1.37183741e-02,  2.25852989e-03,\n",
       "        8.79017171e-03, -5.04150009e-03, -1.11420834e-02,  1.01354253e-02,\n",
       "        2.26622969e-02, -1.54554360e-02, -1.14744401e-03, -2.99247587e-03,\n",
       "       -1.05237961e-02, -4.00301535e-03, -1.06647415e-02,  1.71461608e-02,\n",
       "       -1.16504729e-02,  8.87040980e-03], dtype=float32)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv['machine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "de91596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.3434886634349823),\n",
       " ('a', 0.3294759690761566),\n",
       " ('natural', 0.25562813878059387),\n",
       " ('word2vec', 0.2532128691673279),\n",
       " ('are', 0.24818044900894165),\n",
       " ('processing', 0.2366793304681778),\n",
       " ('and', 0.23120737075805664),\n",
       " ('creating', 0.22531941533088684),\n",
       " ('in', 0.20836141705513),\n",
       " ('high-quality', 0.20542047917842865)]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Most similar to 'machine'\n",
    "\n",
    "cbow_model.wv.most_similar('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46f9db",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. All the above words are the most similar w.r.to the word 'machine' as their vector values are similar w.r.to 'machine'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b8034",
   "metadata": {},
   "source": [
    "### Step 9: To show the list of all the words that the model has learnt for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "b1dfba7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'word': 1,\n",
       " 'are': 2,\n",
       " 'training': 3,\n",
       " 'for': 4,\n",
       " 'models': 5,\n",
       " 'be': 6,\n",
       " 'words': 7,\n",
       " 'of': 8,\n",
       " 'a': 9,\n",
       " 'embeddings': 10,\n",
       " 'learning': 11,\n",
       " 'python': 12,\n",
       " 'in': 13,\n",
       " 'word2vec': 14,\n",
       " 'library': 15,\n",
       " 'popular': 16,\n",
       " 'is': 17,\n",
       " 'gensim': 18,\n",
       " 'datasets': 19,\n",
       " 'text': 20,\n",
       " 'large': 21,\n",
       " 'on': 22,\n",
       " 'by': 23,\n",
       " 'captured': 24,\n",
       " 'can': 25,\n",
       " 'meaning': 26,\n",
       " 'the': 27,\n",
       " 'high-quality': 28,\n",
       " 'creating': 29,\n",
       " 'used': 30,\n",
       " 'deep': 31,\n",
       " 'vectors': 32,\n",
       " 'as': 33,\n",
       " 'represented': 34,\n",
       " 'to': 35,\n",
       " 'allows': 36,\n",
       " 'that': 37,\n",
       " 'representation': 38,\n",
       " 'type': 39,\n",
       " 'fields': 40,\n",
       " 'related': 41,\n",
       " 'closely': 42,\n",
       " 'machine': 43,\n",
       " 'and': 44,\n",
       " 'processing': 45,\n",
       " 'language': 46,\n",
       " 'natural': 47}"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152dc15",
   "metadata": {},
   "source": [
    "### OBSERVATIONS:\n",
    "\n",
    "1. key_to_index ---> It helps in mapping every word present in the vocabulary to its unique integer index that is trained using the Word2Vec Model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
