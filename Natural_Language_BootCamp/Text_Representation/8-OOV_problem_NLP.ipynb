{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### OOV(Out Of Vocabulary) Problem:--\n",
        "\n",
        "OOV is a kind of problem that occurs during the Natural Langauge Processing in which we come across the words that are present in the testing phase but are not present during the training vocabulary dataset.\n",
        "\n",
        "### Simple Example\n",
        "\n",
        "1. Training Data:\n",
        "\n",
        "\"I love data science\"\n",
        "\n",
        "\"Machine learning is amazing\"\n",
        "\n",
        " ##  Vocabulary learned during Training :  I, love, data , science, Machine, Learning, is, amazing.\n",
        "\n",
        "2. Test Sentence:\n",
        "\n",
        "\"I love artificial intelligence\"\n",
        "\n",
        "Here\n",
        "\n",
        "1. artificial\n",
        "\n",
        "2. intelligence\n",
        "\n",
        "The above are the two words that do not occur during the training phase.\n",
        "\n",
        "\n",
        "### Handling OOV Problem in NLP\n",
        "\n",
        "#### Steps used in this Algorithm:-----\n",
        "\n",
        "1.   Import all the necessary libraries\n",
        "\n",
        "2.   define the Training Corpus (Known Words)\n",
        "\n",
        "3.   Create Tokenizer WITHOUT OOV Handling\n",
        "\n",
        "4.   Check Word Index\n",
        "\n",
        "5.   Test the Sentence with Unknown Word\n",
        "\n",
        "#### Let's Solve OOV Problem\n",
        "\n",
        "6.    Create Tokenizer WITH OOV Handling\n",
        "\n",
        "7.    Check Word Index Again\n",
        "\n",
        "8.    Convert Test Sentence Again\n",
        "\n",
        "9.    Perform the padding on  the Sequences"
      ],
      "metadata": {
        "id": "suWe6noAIiyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import all the necessary libraries"
      ],
      "metadata": {
        "id": "u98slmvQLPQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "DpV7Ler5LSzY"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1.  tensorflow ----------------->  Deep Learning Framework\n",
        "\n",
        "2.  Tokenizer  ----------------->  breaks the text sequences into smaller parts\n",
        "\n",
        "3.  sequence   ----------------->  comprises of the input sequences\n",
        "\n",
        "4.  pad_sequences -------------->  performs the padding on the input sequences to make the length as equal.\n"
      ],
      "metadata": {
        "id": "j6nwy3o_L5rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: define the Training Corpus (Known Words)"
      ],
      "metadata": {
        "id": "zJ0mokSYMUSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = [\n",
        "    \"I love data science\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"Deep learning is powerful\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "2sWCLN2_MWGr"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1. This training corpus has three sentences.\n",
        "\n",
        "2.  The tokenizer will learn all the vocabularies from these corpus."
      ],
      "metadata": {
        "id": "aIebZ2HoMbB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create Tokenizer WITHOUT OOV Handling"
      ],
      "metadata": {
        "id": "Ij7ib0ISMrlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create an object of Tokenizer without OOV\n",
        "\n",
        "tokenizer_no_oov = Tokenizer()\n",
        "\n",
        "### using the object of Tokenizer, train and transform the text\n",
        "\n",
        "tokenizer_no_oov.fit_on_texts(train_corpus)"
      ],
      "metadata": {
        "id": "NiZJxG28Mvwe"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1. The object for Tokenizer is created without including OOV(any unknown word will be lost which leads to the loss of information for the Model).\n",
        "\n",
        "2. Then using the object of Tokenizer, fit_on_texts is applied on the corpus data to transform the data and build the vocabulary dictionary.\n",
        "\n",
        "3. Here every word has been assigned with the integer sequence."
      ],
      "metadata": {
        "id": "JUiOaosANLwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4:  Check Word Index\n"
      ],
      "metadata": {
        "id": "0GO4RzrFNsFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_no_oov.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3MKj3TRNtyV",
        "outputId": "772aea7e-4137-42b7-f696-e75761ccf3bd"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning': 1,\n",
              " 'is': 2,\n",
              " 'i': 3,\n",
              " 'love': 4,\n",
              " 'data': 5,\n",
              " 'science': 6,\n",
              " 'machine': 7,\n",
              " 'amazing': 8,\n",
              " 'deep': 9,\n",
              " 'powerful': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1.  Here index is assigned to every word in the text."
      ],
      "metadata": {
        "id": "s68wW30RNxfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Test the Sentence with Unknown Word"
      ],
      "metadata": {
        "id": "4rFlz1kcN5m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = [\"I love artificial intelligence\"]\n",
        "\n",
        "test_sequences_no_oov = tokenizer_no_oov.texts_to_sequences(test_sentence)\n",
        "\n",
        "print(test_sequences_no_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mFU3IFTN7Ul",
        "outputId": "17b2377f-4fa3-4b75-c2ab-04063e828043"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1. texts_to_sequences   -------> It converts the text data into numerical sequences based on the vocabulary learned during the model training.\n",
        "\n",
        "2. Here on applying texts_to_sequences on the test data, it is seen that\n",
        "\n",
        "    (a.) \"artificial\" and  \"intelligence\" are not present in the tarining data\n",
        "\n",
        "    (b.) These are the unknown words that get dissappear, so the Model loses these information.\n",
        "\n",
        "    (c.) This is OOV problem."
      ],
      "metadata": {
        "id": "vT7shJ5LOYvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's Solve OOV Problem\n",
        "\n",
        "#### Step 6:  Create Tokenizer WITH OOV Handling"
      ],
      "metadata": {
        "id": "e_tmWKovPXT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the object for Tokenizer with oov token to identify the unknown words\n",
        "tokenizer_with_oov = Tokenizer(oov_token = \"<OOV>\")\n",
        "### train and transform the corpus text data using the new object for Tokenizer\n",
        "tokenizer_with_oov.fit_on_texts(train_corpus)"
      ],
      "metadata": {
        "id": "-sCfFFo8Pd3t"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1. The object for Tokenizer is created  including OOV(that will identify unknown words that occur during the testing phase and will be marked as OOV).\n",
        "\n",
        "2. Then using the object of Tokenizer, fit_on_texts is applied on the corpus data to transform the data and build the vocabulary dictionary.\n",
        "\n",
        "3. Here every word has been assigned with the integer sequence."
      ],
      "metadata": {
        "id": "KxciCBMkQBHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7:  Check Word Index Again"
      ],
      "metadata": {
        "id": "QkYoKP9jQTDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_with_oov.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga2TkrJyQWLL",
        "outputId": "11ce6ed2-6535-4748-c447-19f09fb21883"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'learning': 2,\n",
              " 'is': 3,\n",
              " 'i': 4,\n",
              " 'love': 5,\n",
              " 'data': 6,\n",
              " 'science': 7,\n",
              " 'machine': 8,\n",
              " 'amazing': 9,\n",
              " 'deep': 10,\n",
              " 'powerful': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1.  Here index is assigned to every word in the text."
      ],
      "metadata": {
        "id": "NMz3A-XrQbG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8:  Convert Test Sentence Again"
      ],
      "metadata": {
        "id": "lhzaCNAOQgxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = [\"I love artificial intelligence\"]\n",
        "\n",
        "test_sequences_with_oov = tokenizer_with_oov.texts_to_sequences(test_sentence)\n",
        "\n",
        "print(test_sequences_with_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1O6a5A0Qjm7",
        "outputId": "8d72526e-2d4f-4a29-e59f-a511f4c949b8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 5, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1.  'artificial' and 'intelligence' are the two unknown words that occur druing the testing but not in model training.\n",
        "\n",
        "2. So now these words will be marked as OOV with index 1 as mentioned in the vocabulary dictionary.\n",
        "\n",
        "3. Now these words will not get dissappeared and the model will not loose any information."
      ],
      "metadata": {
        "id": "xR5GrpjSQ5Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9:  Perform the padding on  the Sequences"
      ],
      "metadata": {
        "id": "n_k_w5nXRVjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padding_sequence = pad_sequences(test_sequences_with_oov, maxlen=6, padding='post')"
      ],
      "metadata": {
        "id": "RepEU7SFRXdC"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SwXB7xLRj3N",
        "outputId": "c6e786ec-2744-4ae5-f42d-642d706dfbbf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4, 5, 1, 1, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OBSERVATIONS:\n",
        "\n",
        "1. Now padding is applied on the test output data.\n",
        "\n",
        "2. It length is 6\n",
        "\n",
        "3. 0 has been added at the end of the sequence due to post  padding."
      ],
      "metadata": {
        "id": "vySgh0uDRmln"
      }
    }
  ]
}